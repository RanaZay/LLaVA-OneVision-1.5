W1222 22:03:02.119000 112091 site-packages/torch/distributed/run.py:803] 
W1222 22:03:02.119000 112091 site-packages/torch/distributed/run.py:803] *****************************************
W1222 22:03:02.119000 112091 site-packages/torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1222 22:03:02.119000 112091 site-packages/torch/distributed/run.py:803] *****************************************
False
False
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/training/utils.py:20: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/training/utils.py:20: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_l2norm
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale
  warnings.warn(
INFO:datasets:PyTorch version 2.9.1 available.
INFO:datasets:PyTorch version 2.9.1 available.
WARNING:megatron.core.transformer.moe.fused_a2a:TE_DType import failed, FP8 communication will be disabled: No module named 'transformer_engine.pytorch'
WARNING:megatron.core.transformer.moe.fused_a2a:Float8BlockQuantizer, Float8BlockwiseQTensor import failed, FP8 not available: No module named 'transformer_engine.pytorch'
WARNING:megatron.core.transformer.moe.fused_a2a:TE_DType import failed, FP8 communication will be disabled: No module named 'transformer_engine.pytorch'
WARNING:megatron.core.transformer.moe.fused_a2a:Float8BlockQuantizer, Float8BlockwiseQTensor import failed, FP8 not available: No module named 'transformer_engine.pytorch'
-------------- Configure model to llava-ov-1.5-4b --------------
  num_layers = 36 
  hidden_size = 2560 
  ffn_hidden_size = 9728 
  num_attention_heads = 32 
  group_query_attention = True 
  num_query_groups = 8 
  position_embedding_type = rope 
  add_position_embedding = False 
  rotary_interleaved = False 
  normalization = RMSNorm 
  swiglu = True 
  attention_dropout = 0 
  hidden_dropout = 0 
  add_bias_linear = False 
  add_qkv_bias = False 
  qk_layernorm = True 
  untie_embeddings_and_output_weights = True 
  vocab_size_in_config_file = 151936 
  make_vocab_size_divisible_by = 128 
  norm_epsilon = 1e-06 
  rotary_base = 5000000 
  kv_channels = 128 
  num_experts = None 
  moe_ffn_hidden_size = None 
---------------- End of configuration ----------------
INFO: Set dataloader type to external since --training-phase=SFT
WARNING: --sft-dataset-config is not specified, setup to default config (/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/configs/sft_dataset_config.json)
using world size: 2, data-parallel size: 1, context-parallel size: 1, hierarchical context-parallel sizes: Nonetensor-model-parallel size: 2, encoder-tensor-model-parallel size: 0, pipeline-model-parallel size: 1, encoder-pipeline-model-parallel size: 0
Number of virtual stages per pipeline stage: None
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/training/arguments.py:741: UserWarning: Using async gradient all reduce requires setting the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1
  warnings.warn(
/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/training/arguments.py:741: UserWarning: Using async gradient all reduce requires setting the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1
  warnings.warn(
------------------------ arguments ------------------------
  account_for_embedding_in_pipeline_split ......... False
  account_for_loss_in_pipeline_split .............. False
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.99
  adam_eps ........................................ 1e-05
  add_bias_linear ................................. False
  add_position_embedding .......................... False
  add_qkv_bias .................................... False
  add_question_in_pretrain ........................ False
  additional_special_tokens ....................... None
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_backend ............................... AttnBackend.flash
  attention_dropout ............................... 0
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calc_ft_timeouts ................................ False
  calculate_per_token_loss ........................ False
  caption_channels ................................ None
  chat_template ................................... qwen2-vl
  check_for_large_grads ........................... False
  check_for_nan_in_loss_and_grad .................. True
  check_for_spiky_loss ............................ False
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch
  ckpt_fully_parallel_load ........................ True
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  combined_1f1b ................................... False
  combined_1f1b_recipe ............................ ep_a2a
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  context_parallel_ulysses_degree ................. 1
  cp_comm_type .................................... ['p2p']
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  cuda_graph_warmup_steps ......................... 3
  custom_pipeline_layers .......................... None
  custom_pipeline_recompute_layers ................ None
  d2d_max_data_volume ............................. 0
  d2d_optimizer ................................... disabled
  data_args_path .................................. None
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_sharding_strategy ................. no_shard
  data_parallel_size .............................. 1
  data_path ....................................... ['/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/data/LLaVA-558K-Webdataset']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. stage_1_alignment_llava_ov_4b/dataloader
  dataloader_type ................................. external
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  ddp_num_buckets ................................. None
  ddp_pad_buckets_for_high_nccl_busbw ............. False
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  dense_mlp_activation_func_recompute ............. False
  deprecated_use_mcore_models ..................... False
  detail_log_interval ............................. 20
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ema_decay ....................................... 0.9999
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_cuda_graph ............................... False
  enable_discard_sample ........................... False
  enable_ema ...................................... False
  enable_fa_within_mla ............................ False
  enable_fp8_comm ................................. False
  enable_ft_package ............................... False
  enable_gloo_process_groups ...................... True
  enable_mem_monitor .............................. False
  enable_one_logger ............................... False
  enable_turn_off_bucketing ....................... True
  encoder_num_layers .............................. 36
  encoder_pipeline_model_parallel_size ............ 0
  encoder_seq_length .............................. 4096
  encoder_tensor_model_parallel_size .............. 0
  end_weight_decay ................................ 0.0
  eod_mask_loss ................................... False
  error_injection_rate ............................ 0
  error_injection_type ............................ transient_error
  eval_interval ................................... 1000
  eval_iters ...................................... 100
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  exp_avg_dtype ................................... torch.float32
  exp_avg_sq_dtype ................................ torch.float32
  expert_model_parallel_size ...................... 1
  expert_tensor_parallel_size ..................... 2
  ffn_hidden_size ................................. 9728
  finetune ........................................ False
  first_last_layers_bf16 .......................... False
  flash_decode .................................... False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_recipe ...................................... delayed
  fp8_wgrad ....................................... True
  fps ............................................. 2.0
  fps_max_frames .................................. 768
  fps_min_frames .................................. 4
  frame_max_pixels ................................ 602112
  frame_min_pixels ................................ 100352
  global_batch_size ............................... 2
  grad_reduce_in_bf16 ............................. False
  gradient_accumulation_fusion .................... False
  gradient_reduce_div_fusion ...................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hf_tokenizer_path ............................... /l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/checkpoints/LLaVA-OneVision-1.5-4B-stage0
  hidden_dropout .................................. 0
  hidden_size ..................................... 2560
  hierarchical_context_parallel_sizes ............. None
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  image_resolution ................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... -1
  inference_max_batch_size ........................ 8
  inference_max_seq_length ........................ 2560
  inference_rng_tracker ........................... False
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  init_model_with_meta_device ..................... False
  initial_loss_scale .............................. 65536.0
  is_tokenized_data ............................... False
  iter_per_epoch .................................. 1250
  iterations_to_skip .............................. []
  keep_fp8_transpose_cache_when_using_custom_fsdp . False
  kv_channels ..................................... 128
  kv_lora_rank .................................... 32
  language_model_type ............................. None
  latent_frame_interval ........................... 1
  latent_in_channels .............................. None
  latent_out_channels ............................. None
  latent_patch_size ............................... (1, 1, 1)
  latent_space_scale .............................. 1.0
  latent_time_scale ............................... 1.0
  layernorm_recompute ............................. False
  lazy_mpu_init ................................... None
  length_sort_desc ................................ False
  length_sort_pool_size ........................... 0
  load ............................................ /l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/checkpoints/LLaVA-OneVision-1.5-4B-stage0_mcore_tp2_pp1
  load_ema ........................................ None
  local_rank ...................................... 0
  log_detail ...................................... True
  log_interval .................................... 1
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.0001
  lr_decay_iters .................................. 5
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.002
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  main_grads_dtype ................................ torch.float32
  main_params_dtype ............................... torch.float32
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_latent_height ............................... None
  max_latent_width ................................ None
  max_pixels ...................................... 12845056
  max_position_embeddings ......................... 32768
  max_text_length ................................. None
  max_tokens_to_oom ............................... 12000
  mem_monitor_force_print_token_threshold ......... 10000000
  mem_monitor_log ................................. None
  memory_snapshot_path ............................ snapshot.pickle
  merge_file ...................................... None
  micro_batch_size ................................ 1
  microbatch_group_size_per_vp_stage .............. None
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  min_pixels ...................................... 3136
  mla_recompute ................................... False
  mmap_bin_files .................................. True
  mock_data ....................................... False
  model_family .................................... llava_ov_1_5
  model_name ...................................... llava-ov-1.5-4b
  moe_aux_loss_coeff .............................. 0.0
  moe_enable_deepep ............................... False
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_ffn_hidden_size ............................. None
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_freq .................................. 1
  moe_layer_recompute ............................. False
  moe_mlp_activation_func_recompute ............... False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_permute_fusion .............................. False
  moe_router_bias_update_rate ..................... 0.001
  moe_router_dtype ................................ None
  moe_router_enable_expert_bias ................... False
  moe_router_force_load_balancing ................. False
  moe_router_group_topk ........................... None
  moe_router_load_balancing_type .................. aux_loss
  moe_router_num_groups ........................... None
  moe_router_padding_for_fp8 ...................... False
  moe_router_pre_softmax .......................... False
  moe_router_score_function ....................... softmax
  moe_router_topk ................................. 2
  moe_router_topk_scaling_factor .................. None
  moe_shared_expert_intermediate_size ............. None
  moe_shared_expert_overlap ....................... False
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_legacy_grouped_gemm ..................... False
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  mscale .......................................... 1.0
  mscale_all_dim .................................. 1.0
  mtp_loss_coef ................................... 0.1
  multi_latent_attention .......................... False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-06
  normalization ................................... RMSNorm
  num_attention_heads ............................. 32
  num_bucket_build_workers ........................ 1
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_distributed_optimizer_instances ............. 1
  num_experts ..................................... None
  num_latent_frames ............................... None
  num_layers ...................................... 36
  num_layers_at_end_in_bf16 ....................... 1
  num_layers_at_start_in_bf16 ..................... 1
  num_layers_per_virtual_pipeline_stage ........... None
  num_nextn_predict_layers ........................ 0
  num_query_groups ................................ 8
  num_virtual_stages_per_pipeline_rank ............ None
  num_workers ..................................... 1
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  optimizer_cpu_offload ........................... False
  optimizer_offload_fraction ...................... 1.0
  output_bert_embeddings .......................... False
  overlap_cpu_optimizer_d2h_h2d ................... False
  overlap_d2d_optimizer ........................... True
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_p2p_comm_warmup_flush ................... False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  packing_batch_size .............................. None
  packing_pretrain_data ........................... False
  packing_sft_data ................................ False
  padded_vocab_size ............................... None
  padding_side .................................... right
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  per_split_data_args_path ........................ None
  perform_initialization .......................... True
  pin_cpu_grads ................................... True
  pin_cpu_params .................................. True
  pipeline_model_parallel_comm_backend ............ None
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  pretrained_checkpoint ........................... None
  print_mem_monitor_interval ...................... 100000
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  q_lora_rank ..................................... None
  qk_head_dim ..................................... 128
  qk_layernorm .................................... True
  qk_pos_emb_head_dim ............................. 64
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... full
  recompute_method ................................ uniform
  recompute_num_layers ............................ 4
  record_memory_history ........................... False
  reduced_data_volume_from_pre_stage .............. 0
  relative_attention_max_distance ................. 128
  relative_attention_num_buckets .................. 32
  replication ..................................... False
  replication_factor .............................. 2
  replication_jump ................................ None
  rerun_mode ...................................... disabled
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  result_rejected_tracker_filename ................ None
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rope_in_fp32 .................................... True
  rope_scaling_factor ............................. 8.0
  rotary_base ..................................... 5000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_scaling_factor ........................... 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ stage_1_alignment_llava_ov_4b
  save_ema ........................................ None
  save_interval ................................... 2000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  separate_layernorm_and_collinear ................ False
  seq_length ...................................... 4096
  sequence_parallel ............................... False
  sft_data_mix_strategy ........................... concat
  sft_data_streaming .............................. False
  sft_dataset ..................................... None
  sft_dataset_config .............................. /l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/configs/sft_dataset_config.json
  sft_num_preprocess_workers ...................... None
  sft_sort_batch .................................. False
  sft_test_dataset ................................ None
  sft_train_dataset ............................... None
  sft_valid_dataset ............................... None
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  split_bw ........................................ False
  split_special_tokens ............................ False
  squared_relu .................................... False
  start_weight_decay .............................. 0.0
  stdit_bucket_config ............................. None
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  streaming_buffer_size ........................... 16384
  suggested_communication_unit_size ............... 400000000
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  te_rng_tracker .................................. False
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. stage_1_alignment_llava_ov_4b/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. HFTokenizer
  tp_comm_bootstrap_backend ....................... nccl
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 5
  train_on_prompt ................................. False
  train_samples ................................... None
  train_sync_interval ............................. None
  trainable_modules ............................... ['adapter']
  training_phase .................................. sft
  training_rice_vl_max_answer_length .............. 4096
  training_rice_vl_max_image_area ................. 1806336
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. True
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_custom_fsdp ................................. False
  use_dist_ckpt ................................... False
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... True
  use_fast_tokenizer .............................. False
  use_flash_attn .................................. False
  use_legacy_models ............................... False
  use_mp_args_from_checkpoint_args ................ False
  use_normhead .................................... False
  use_one_sent_docs ............................... False
  use_persistent_ckpt_worker ...................... False
  use_precision_aware_optimizer ................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rope_scaling ................................ False
  use_rotary_position_embeddings .................. False
  use_tokenizer_model_from_checkpoint_args ........ True
  use_torch_fsdp2 ................................. False
  use_torch_optimizer_for_cpu_offload ............. False
  use_tp_pp_dp_mapping ............................ False
  v_head_dim ...................................... 128
  valid_data_path ................................. None
  variable_seq_lengths ............................ True
  video_max_pixels ................................ 51380224
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  vocab_size_in_config_file ....................... 151936
  vpp_scheduler ................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.0
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 2
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 2
> AIAK building HFTokenizer tokenizer ...
WARNING: tokenizer already has an EOS token, replace <|im_end|> with <|im_end|>, and will add 0 new tokens to tokenizer.
 > padded vocab (size: 151936) with 128 dummy tokens (new size: 152064)
WARNING:megatron.core.rerun_state_machine:RerunStateMachine initialized in mode disabled
> initializing torch distributed ...
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_megatron/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.056 seconds
> compiling and loading fused kernels ...
/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1222 22:03:46.728785705 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
> setting tensorboard ...
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
>>> done with compiling and loading fused kernels. Compilation time: 0.921 seconds
time to initialize megatron (seconds): 12.880
/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[after megatron is initialized] datetime: 2025-12-22 22:03:57 
building llava-ov-1.5-4b model ...
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
3 False
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 4201988608
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 4201988608
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.99)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-05
    foreach: None
    fused: None
    is_decoupled_lr: False
    is_expert_parallel: False
    lr: 0.0001
    lr_mult: 1.0
    max_lr: 0.0001
    maximize: False
    min_lr: 1e-06
    wd_mult: 0.0
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.99)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-05
    foreach: None
    fused: None
    is_decoupled_lr: False
    is_expert_parallel: False
    lr: 0.0001
    lr_mult: 1.0
    max_lr: 0.0001
    maximize: False
    min_lr: 1e-06
    wd_mult: 1.0
    weight_decay: 0.0
)
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=True, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, check_for_large_grads=False, bucket_size=None, pad_buckets_for_high_nccl_busbw=False, average_in_collective=False, fp8_param_gather=False, use_custom_fsdp=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, suggested_communication_unit_size=400000000, preserve_fp32_weights=True, keep_fp8_transpose_cache_when_using_custom_fsdp=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (27271680 elements, 27271680 padded size):
	module.adapter.linear_fc1.linear.bias
	module.adapter.layernorm.bias
	module.adapter.linear_fc2.linear.weight
	module.adapter.linear_fc1.linear.weight
	module.adapter.layernorm.weight
	module.adapter.linear_fc2.linear.bias
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0001, min_lr=1e-06, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.0, fp8_recipe='delayed', fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=65536.0, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.99, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, optimizer_cpu_offload=False, optimizer_offload_fraction=1.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x152e5a77f490>, config_logger_dir='', d2d_optimizer='disabled', overlap_d2d_optimizer=True, d2d_max_data_volume=0, reduced_data_volume_from_pre_stage=0)
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.99)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-05
    foreach: None
    fused: None
    is_decoupled_lr: False
    is_expert_parallel: False
    lr: 0.0001
    lr_mult: 1.0
    max_lr: 0.0001
    maximize: False
    min_lr: 1e-06
    wd_mult: 0.0
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.99)
    capturable: False
    decoupled_weight_decay: True
    differentiable: False
    eps: 1e-05
    foreach: None
    fused: None
    is_decoupled_lr: False
    is_expert_parallel: False
    lr: 0.0001
    lr_mult: 1.0
    max_lr: 0.0001
    maximize: False
    min_lr: 1e-06
    wd_mult: 1.0
    weight_decay: 0.0
)
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
 loading release checkpoint from /l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/checkpoints/LLaVA-OneVision-1.5-4B-stage0_mcore_tp2_pp1
Loading checkpoint with device: cuda:0, strict=False
 checkpoint version 3.0
  successfully loaded checkpoint from /l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/checkpoints/LLaVA-OneVision-1.5-4B-stage0_mcore_tp2_pp1 [ t 1/2, p 1/1 ] at iteration 0
(min, max) time across ranks (ms):
    load-checkpoint ................................: (7090.80, 7090.82)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-12-22 22:04:44 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      10
    validation: 200
    test:       200
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.
rank=0, worker=0: shard_range=[pretrain-000003.tar[0, 5058), pretrain-000001.tar[0, 5036), pretrain-000002.tar[0, 5039), pretrain-000004.tar[0, 3821), pretrain-000000.tar[0, 5034)] sum(count)=23988
rank=0, worker=0: shard_range=[pretrain-000003.tar[0, 5058), pretrain-000001.tar[0, 5036), pretrain-000002.tar[0, 5039), pretrain-000004.tar[0, 3821), pretrain-000000.tar[0, 5034)] sum(count)=23988
dataset state stage_1_alignment_llava_ov_4b/dataloader/iter_0000000/mp_rank_00/train_dataloader_dprank000.pt does not exist
dataset state stage_1_alignment_llava_ov_4b/dataloader/iter_0000000/mp_rank_01/train_dataloader_dprank000.pt does not exist
[after dataloaders are built] datetime: 2025-12-22 22:04:45 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (46747.78, 46747.84)
    train/valid/test-data-iterators-setup ..........: (1066.80, 1066.81)training ...

Setting rerun_state_machine.current_iteration to 0...
[before the start of training step] datetime: 2025-12-22 22:04:45 
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00013874', __restore_key__=('MapDataset', 0, 'Webdataset', 'pretrain-000002.tar', 0), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...walk, as well as other figures', 'woman vintage leather patchwork handbags ladies totes bags large capacity ladies shoulder bags fashion messenger bag purse female', 'sting springsteen framed print'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00000708', __restore_key__=('MapDataset', 1, 'Webdataset', 'pretrain-000004.tar', 0), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...phy', 'three different colors cats are walking on stairs', 'boys soccer game played in the central maine community area on september 13th, 2019', 'a bunch of twitter boards that are different sizes'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00019328', __restore_key__=('MapDataset', 2, 'Webdataset', 'pretrain-000004.tar', 1), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...mney and bach greet supporters at the republican presidential convention on tuesday', "celtic manager brendan o'neill looking on during the scottish league match between corks and celtic at firness"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00008790', __restore_key__=('MapDataset', 3, 'Webdataset', 'pretrain-000004.tar', 2), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...e topped with a sand tray filled with sand', 'a turkish flag in the wind greeting card by martin tszan', 'the girls hockey team in the throwback thursday banner', "a flower men's v - neck t - shirt"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00012279', __restore_key__=('MapDataset', 4, 'Webdataset', 'pretrain-000000.tar', 0), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...text says, are you a woman? i am a', 'a man holding a ladder on top of a reformer machine', 'silver unicorn pendant with multicolor austrian crystals on a chain bracelet', 'diamond bracelet diamond'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00013099', __restore_key__=('MapDataset', 5, 'Webdataset', 'pretrain-000003.tar', 0), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...turtle neck sweaters casual loose jumper top", 'pokemon - version negere 2, nintendo ds', 'the emiicides  /      ', 'the blue and orange paint washable brush bag by watercoln studio'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00012534', __restore_key__=('MapDataset', 6, 'Webdataset', 'pretrain-000002.tar', 1), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I... cord and speaker', 'elk lighting 66381bk - bwv / gd 1 light outdoor wall light in', 'two bibshorts with the same design on them', 'dragon age 2 xbox36, the', 'nyx cosmetics makeup stick in butter'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00005236', __restore_key__=('MapDataset', 7, 'Webdataset', 'pretrain-000000.tar', 1), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...ly bones by alice sebold', 'a popsicle with the words fun and chill on it, painted in bright blue colors', 'anusha baneshy, manisha kol and raveera gupta', 'a libyan shoe maker sits in his workshop'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00007060', __restore_key__=('MapDataset', 8, 'Webdataset', 'pretrain-000001.tar', 0), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...ner in the room of the modellasess user with its premipifier', 'the front of a white bmw 6 - series convertible parked in the middle of a street', 'emelita performs with her children at the concert'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00012417', __restore_key__=('MapDataset', 9, 'Webdataset', 'pretrain-000002.tar', 2), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.I...image of butterfly design bracelet in gold tone', 'a women with a sleek, layered bob cut', 'a snow globe shaped like a photo of a small harbor and the town of padstow, cornwall', 'athlete salt soap'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00001765', __restore_key__=('MapDataset', 10, 'Webdataset', 'pretrain-000004.tar', 3), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....nger'57mm cat eye sunglasses", 'mike wheeler is back in action with the kick by a ufc opponent at the ufc fight in melbourne', 'the entrance to town square theatre, at the universal studios and mgm'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00008635', __restore_key__=('MapDataset', 11, 'Webdataset', 'pretrain-000001.tar', 1), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...., 'printer hp ink advantage 2510', 'a simple wedding dress with a lace bodicement and a chapel train', "girls'varsity team with plaque and gold plaque", 'vintage party set of retro hipster elements'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00021780', __restore_key__=('MapDataset', 12, 'Webdataset', 'pretrain-000004.tar', 4), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....rties', 'marvel vs capcom ultimate', 'a assassins creed screenshot looking at a city', 'the sand wedge of the 12th green at royal melbourne golf club', 'splendid sneakers with double buckle closure'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009092', __restore_key__=('MapDataset', 13, 'Webdataset', 'pretrain-000000.tar', 2), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... a sign that says student voices matter', 'a blue modern hatchback car against a white background, shot with a shallow focus lens on the front -', 'red and blue statement necklace, handmade jewelry'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00016165', __restore_key__=('MapDataset', 14, 'Webdataset', 'pretrain-000004.tar', 5), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....women's muscle tank", 'the church is a hospital for sinns not a museum for saints', 'a cappella by chapo maived at la collection ramaban', 'grilled fish tacos with red cabbage slaw and fresh cilant'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00017622', __restore_key__=('MapDataset', 15, 'Webdataset', 'pretrain-000002.tar', 3), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....o for marvel's agents of shieldo", 'minnie mouse cupcake kit', 'a white sneaker with check ribbon and heel', "men's pant - hunting - pants with kneepads", 'an electronic herb grind with an open lid'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00002980', __restore_key__=('MapDataset', 16, 'Webdataset', 'pretrain-000002.tar', 4), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....an war, with online learning tools', 'a pair of red wing shoes in brown and brown leather', 'a child making a letter puzzle with post it notes', 'the agents agents, with stephen clarke, on the left'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00007798', __restore_key__=('MapDataset', 17, 'Webdataset', 'pretrain-000001.tar', 2), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....of a desk with the words video about seo specialists', 'schaum patriotic solos for flute primer level book with cd, audio online', 'the flyer for the conversation on whether politics is transfusion'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00014456', __restore_key__=('MapDataset', 18, 'Webdataset', 'pretrain-000001.tar', 3), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ce with the words refurbished in red and white', 'the tag watches are very attractive and stylish they look like the real one', 'a blue stuffed animal character mascot with a toothbrush on its nose'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00020966', __restore_key__=('MapDataset', 19, 'Webdataset', 'pretrain-000002.tar', 5), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... a courtroom', 'the back of a man who has some sunburn', 'turkish president person addresses at a parliament meeting in ankara', 'windows with sky light and blue door at the end of a brick building'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00006856', __restore_key__=('MapDataset', 20, 'Webdataset', 'pretrain-000002.tar', 6), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... 'a number of objects and services that are required in nanosystems for nanosystems', 'a worker serves two mcdonalds meals', "a man holding a piece of paper with the text'global english lesson 03 '"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00002790', __restore_key__=('MapDataset', 21, 'Webdataset', 'pretrain-000004.tar', 6), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....'the statue of hindu god in a temple stock photo, an image of the statue of hindu lord vishnu in', 'eames dsw side chair, wood base - brown', "the boys'soccer team is ready to celebrate after a win"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00013144', __restore_key__=('MapDataset', 22, 'Webdataset', 'pretrain-000002.tar', 7), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....elves on wheels', 'the queen beetles cello player with violin on yellow plinte - 8cm / 4 5 inch', 'comet engine gasket kit - 1987 honda cr120 wiseco pro - lit 2 - stroker', 'mac metallic eye shadow'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00008243', __restore_key__=('MapDataset', 23, 'Webdataset', 'pretrain-000002.tar', 8), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ougar climbing into the tree at the zoo, in the animal enclosure', 'a research proposal letter for a candidate for the next session', 'a close up of a custom shop putter in silver, white and yellow'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002429', __restore_key__=('MapDataset', 24, 'Webdataset', 'pretrain-000003.tar', 1), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....matterhorn observatory building is on top of the matterhorn mountain', 'the weather service in montenegro and romania', 'the james webb telescope, with its yellow reflect shield and one solar probe'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00012098', __restore_key__=('MapDataset', 25, 'Webdataset', 'pretrain-000004.tar', 7), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ket in black, with a yellow zipper in the chest', "a women's shortie shorts with pockets in a bright red color", "usa's gabby coleman, left, and kenya napier compete in the women's 400 - meter race"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00011763', __restore_key__=('MapDataset', 26, 'Webdataset', 'pretrain-000001.tar', 4), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ul piece of wall art', 'a street sign that says confront fears', 'illustrated bible dictionary for kids', 'the front of a white softball t - shirt that has the cleveland indians drawn in the design'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00022854', __restore_key__=('MapDataset', 27, 'Webdataset', 'pretrain-000002.tar', 9), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....perched on a branch', 'white wall and red door in a room with a vase royalty illustration', 'shrimp tails are stacked on ice, with some on the side', 'a woman waves while riding a boat in the ocean'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00019838', __restore_key__=('MapDataset', 28, 'Webdataset', 'pretrain-000000.tar', 3), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... of a catechhismm of the catholic church, with an", 'the best war in the world - hacker vs ranker game', 'a handcuffed cash banknotes on handcuffs', '20 % off on full - priced items from mothercare'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00011917', __restore_key__=('MapDataset', 29, 'Webdataset', 'pretrain-000004.tar', 8), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ords you don't have to be crazy to work here we'll train", 'a car sale ad with the text carolina state inspection valid at participating mcc locations', 'a small halloween pennant on a wooden fence'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00014561', __restore_key__=('MapDataset', 30, 'Webdataset', 'pretrain-000004.tar', 9), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....logo pullover hoodie sweatshirt", "3 pairs shorts - fashion women's casual high waist elasticated waistband shorts", 'bosch pdc - 350 percussion drills and drill set with impactor and carrying case'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00004801', __restore_key__=('MapDataset', 31, 'Webdataset', 'pretrain-000002.tar', 10), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ns behind it', 'two dug dugies swim under water at an underwaterpark', 'australia day instagram banner templates', 'moses lake school district logo', 'amanda miller in the fourth annual polo outing'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00005266', __restore_key__=('MapDataset', 32, 'Webdataset', 'pretrain-000001.tar', 5), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ailboat and its sailers on the river', 'tele 24 led tv', 'happy birthday wallpaper', '1992 - 93 flee reprints 90 joe dumars', 'the ladies of metal festival at the hollywood hotel in los, california'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00008574', __restore_key__=('MapDataset', 33, 'Webdataset', 'pretrain-000002.tar', 11), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...with a line drawing of children in different positions and with a line drawing of an adult child playing', 'a gown with a sweetheart cutout on the sweetheart neck and low back, sequins in the front'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00006751', __restore_key__=('MapDataset', 34, 'Webdataset', 'pretrain-000002.tar', 12), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...irt in grey with pirate face on it', 'eastenders actress, amber tempercote and eastenders star daniel knight are shown', 'three types of network remotes, one is a server and one is an internet user'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00004517', __restore_key__=('MapDataset', 35, 'Webdataset', 'pretrain-000002.tar', 13), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...g face cream', 'a woman rock climbing on a ledge', 'an ocean view suite with sofa and chairs', 'the chapter in a book by charles j stewart with a paragraph on cognition, intelligence and creativity'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00003427', __restore_key__=('MapDataset', 36, 'Webdataset', 'pretrain-000004.tar', 10), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... cloud computing theme stock photo, a picture of a computer keys with cloud computing theme by sergey', 'a black fiat car is shown with black wheels', 'kite surfers out on the water with a red kite'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00019955', __restore_key__=('MapDataset', 37, 'Webdataset', 'pretrain-000002.tar', 14), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...me ever', "the price on review's best value colleges 2014 edition", 'people taking photos around the epcot flower and garden show', 'geraldine brooks quotes i never promised i would write the truth'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00022649', __restore_key__=('MapDataset', 38, 'Webdataset', 'pretrain-000002.tar', 15), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ttle boy with his arm out holding a nintendo wii controller', 'the music star has become the most beautiful girl in the world', 'the campus app shows a photo with students in caps and gowns smiling'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00009555', __restore_key__=('MapDataset', 39, 'Webdataset', 'pretrain-000001.tar', 6), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....nd hat and the title girl', "joy's brown button front blouse on the view", "an electric model of a truck with liverys in the back and the words'man bosch '", 'a pretty little secret by sara shepard'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00018351', __restore_key__=('MapDataset', 40, 'Webdataset', 'pretrain-000000.tar', 4), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....and narrative of the contest which led', 'two pages of pink and white budget tracker with a clothespin', 'young people sit on a cinema chair and look at each other photo', 'the real housewives cast'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00019546', __restore_key__=('MapDataset', 41, 'Webdataset', 'pretrain-000003.tar', 2), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....'the grenfell tower, with the city in the background', "hollywood's favorite actor and super - happy couple matthew lawrence", 'open door to the bright sky and ocean animation on empty room footage'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00012460', __restore_key__=('MapDataset', 42, 'Webdataset', 'pretrain-000004.tar', 11), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... 'rihanna is a face model for fashion magazine harper', "men's raven crew sweater cinder grey", 'an email for a customer to write their support letter', 'the lords of the lord scroll incense burner'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00013503', __restore_key__=('MapDataset', 43, 'Webdataset', 'pretrain-000000.tar', 5), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ball shoes men leather casual outdoor sports basketball shoes', 'a pair of brake discs rotors rotors for mercedes cla, mercedes c4, benz, audi', 'the monkey knife fight logo and fantasy draft logos'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00006780', __restore_key__=('MapDataset', 44, 'Webdataset', 'pretrain-000001.tar', 7), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....go', 'unique air express is a great way to buy air tickets, tickets, discount offers, and more', 'a very rare and attractive empire style ormolu fired bronze figurine mantel clock by french company'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00000353', __restore_key__=('MapDataset', 45, 'Webdataset', 'pretrain-000002.tar', 16), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...n countryside initiative logo', 'discount flyer for alignment special', 'a silver 2019 mercedes gle suv parked outside of a lake', 'the building with the european air force headquarters in brussels'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00013770', __restore_key__=('MapDataset', 46, 'Webdataset', 'pretrain-000002.tar', 17), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...e', 'a russian matryoshka set', 'elephant bracelet, personalized bangle bracelet, initial charm, sterling silver wire bangle, initial jewelry,', 'l - carnitine 100 000 400 tableta biotech nutrition'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00023364', __restore_key__=('MapDataset', 47, 'Webdataset', 'pretrain-000000.tar', 6), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ge area carpet', 'sparkslite on steam from nintendo store', 'two men and a woman in army clothing talking to each other', 'a weather dashboard showing the average temperature and current conditions'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007006', __restore_key__=('MapDataset', 48, 'Webdataset', 'pretrain-000003.tar', 3), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....de in vlad', 'the galaxy s4 with the galaxy note ii, the first in its three color choices', 'an unmarried woman posters for sale', 'a lego lego car with a truck, batmobile, and three other vehicles'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00012014', __restore_key__=('MapDataset', 49, 'Webdataset', 'pretrain-000003.tar', 4), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....d red fishing rod', 'the average wage of people per month to labor force by gender since men and women', 'a poster for kingdom of heaven, a movie about king leonid the leonid of the ancient kingdom'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00022626', __restore_key__=('MapDataset', 50, 'Webdataset', 'pretrain-000000.tar', 7), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....hirt', 'the england cricketer has his bat swung off as they hit out during the cricket world cup pool a match between', 'a screen capture of the book article that has been removed from google books'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00022130', __restore_key__=('MapDataset', 51, 'Webdataset', 'pretrain-000002.tar', 18), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... 'the atletico team celebrate a goal against levantres', 'astra dhabitsu membership badge', 'the subaruna sport - tech car of the year winner', 'a remote control drone that uses multiple propellers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00014111', __restore_key__=('MapDataset', 52, 'Webdataset', 'pretrain-000002.tar', 19), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...rns 0804", 'the north face womens gotham down parka in tn tnw black', 'a lot of pewware including bowl, cup, plate, spoon and bowl', 'lagunamoon 10ml aroma oils diffuser essential oil for oily skin'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00002080', __restore_key__=('MapDataset', 53, 'Webdataset', 'pretrain-000001.tar', 8), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... and wooden flooring', 'a set of three hot air balloon mobiles hanging from strings', 'the dynocare with its engine code number in red', 'the europe poker tour logo is pictured on a blue background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00006169', __restore_key__=('MapDataset', 54, 'Webdataset', 'pretrain-000002.tar', 20), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...an cities on an airport timetable board', 'the cover of the game grid autosport', 'thomas and friends watercolor book with paint', 'a black and white image of the cover of the single from ave maria'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017416', __restore_key__=('MapDataset', 55, 'Webdataset', 'pretrain-000003.tar', 5), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....el tools on white background', 'a green gap logo hoodie with the word gap in arched across the front', 'pink, orange, and brown cork wedge sandals', 'secie wallet card holder brown crocodile effect'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00006956', __restore_key__=('MapDataset', 56, 'Webdataset', 'pretrain-000000.tar', 8), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....y to the catalan language", 'an utility truck parked in front of a house with the united trash removal logo on it', 'the american cancer society estimates that 30 million n cases are linked to stat'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00001468', __restore_key__=('MapDataset', 57, 'Webdataset', 'pretrain-000002.tar', 21), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...e it is an ultra bright light', 'nhl awards 2014 stanley cup winners', 'the logo for river downs golf club', "the aurora glow from nasa's issu - 1a telescope", 'the intel - cpu processor, core i7'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00020052', __restore_key__=('MapDataset', 58, 'Webdataset', 'pretrain-000001.tar', 9), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....y email', 'the presenter is pictured on the ellen williams show', 'a diagram shows different types of business branding', 'a wii version of ape academy', 'the cinderella ballet tour at theatre west'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017850', __restore_key__=('MapDataset', 59, 'Webdataset', 'pretrain-000003.tar', 6), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....ates marine logo round stickers', 'bohemia multicolor beads tassel long earrings', 'the adi hockey shirt features a canadian maple leaf', 'strictly classics book 2', 'the north face apex 2 0 jacket'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00020041', __restore_key__=('MapDataset', 60, 'Webdataset', 'pretrain-000002.tar', 22), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... the un security council debates on the situation', 'the bathroom walls in white are made of porcelain tiles, with a large basin and a circular tub', 'business card design for a home repair company'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017777', __restore_key__=('MapDataset', 61, 'Webdataset', 'pretrain-000000.tar', 9), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....'a gold keychain with the bible on it', "the north face women's stretchdown triclr hoodie black", 'a snow cone maker with a counter top', 'stacking crates plastic folding storage crate with handles'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00012515', __restore_key__=('MapDataset', 62, 'Webdataset', 'pretrain-000000.tar', 10), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...s the euro dollar and euro', 'a blue tick sign or icon in the shape of a check box or check mark with a shadow', 'chef michael grisel of kitchen school', 'a screenshot of the customer feedback page'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00012573', __restore_key__=('MapDataset', 63, 'Webdataset', 'pretrain-000001.tar', 10), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... bunch of characters in mario tennis', 'a painting with figures in a lotus motif and a bird on top of a plant', 'replacement laptop battery for hp envy pavilion elite, pavilion notebook 14 - 1005et'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00003922', __restore_key__=('MapDataset', 64, 'Webdataset', 'pretrain-000002.tar', 23), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...he book the bell jar', 'a screenshot of the video game zombie road', 'whale diving in the ocean off a boat', '4 bedroom detached house for sale in the green, littleborough', 'american flag sneakers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00001330', __restore_key__=('MapDataset', 65, 'Webdataset', 'pretrain-000001.tar', 11), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...bowl of chia pudding next to two bowls filled with chia seeds', 'a model walks the runway at the fashion show during paris menswear', 'a scientist works in his laboratory at the university of otago'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00011927', __restore_key__=('MapDataset', 66, 'Webdataset', 'pretrain-000000.tar', 11), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...", 'the sweater coat in grey', 'a front load load washer in graph steel, featuring a front load perfect steam system', 'avatar, which is a tribute to avatar, is one of the most iconic movie sequels'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00003274', __restore_key__=('MapDataset', 67, 'Webdataset', 'pretrain-000002.tar', 24), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ener', 'image of dior 51mm aviator sunglasses', 'a pdf page of a medical policy document that shows a description of some of the various processes of an in', 'the cover of little farm in the ozarks'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00023124', __restore_key__=('MapDataset', 68, 'Webdataset', 'pretrain-000001.tar', 12), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...in the field and in the woods', 'my own character in the sci - themed city of ark screenshot', 'tata cars to launch new nano suv soon', 'george and stacy have been married for more than three years'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00019512', __restore_key__=('MapDataset', 69, 'Webdataset', 'pretrain-000004.tar', 12), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ston behind the mascot', 'a pie chart showing the percentage of all of the music related product orders available on amazon', 'a hunting gun, and a group of dead animals are on display in the field'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00020179', __restore_key__=('MapDataset', 70, 'Webdataset', 'pretrain-000000.tar', 12), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...n hong, which have been reviewed by hong tourists', 'the british military para gta', 'the racketeer a novel', 'the beautiful dunes golf club at hilton national', 'women perform a zumba on the beach'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00010857', __restore_key__=('MapDataset', 71, 'Webdataset', 'pretrain-000004.tar', 13), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...e slices', 'the settlers 2 game cheat', 'the origin of english place names', 'president john fitzgerald with dog in oval room of white house', 'the model train layout', 'a textile bag from pakistan'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00013282', __restore_key__=('MapDataset', 72, 'Webdataset', 'pretrain-000004.tar', 14), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... 'the new trend of embroidery restoring small bag lady handbags women shoulder bag messenger small fresh fashion ladies shoulder bags', 'an illustration of the word basketball on a white background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00019564', __restore_key__=('MapDataset', 73, 'Webdataset', 'pretrain-000003.tar', 7), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....smartphone showing a product list for crayols, toilet paper and more', 'a shirt with the words system of a down on it', 'blonde girl holding the flag of democratic democratic democratic democratice'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00015653', __restore_key__=('MapDataset', 74, 'Webdataset', 'pretrain-000000.tar', 13), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...thy fajita recipes for kids', 'a white stem glass with the name of the team and logo', 'an oval portraiture of a young woman holding a bird and a book', 'the simpsons veggietales plush toy keychain'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00020618', __restore_key__=('MapDataset', 75, 'Webdataset', 'pretrain-000001.tar', 13), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...age from the side', 'customer success without cultural adoption', 'a large projection image of john f kennedy is projected on a building in downtown seattle', 'a building is seen in the mumbai area'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00010998', __restore_key__=('MapDataset', 76, 'Webdataset', 'pretrain-000000.tar', 14), __subflavor__=None, __subflavors__={}, images=[<PIL.Image..., 'four fabric fat quarter squares in gray, yellow, and white', 'the green county sheriff logo is shown in a file cabinet', 'they whine wine wine quote for alcohol enthusiasts - mousepad horizontal'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00013096', __restore_key__=('MapDataset', 77, 'Webdataset', 'pretrain-000002.tar', 25), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...aker 2 way 5 ohm car speakers 5w - 10w speaker 4ohm waterproof', 'the ballad style of stan kenton by stan kenton', 'a child in a white sweatshirt with his hands on his back with a science t - shirt'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00020652', __restore_key__=('MapDataset', 78, 'Webdataset', 'pretrain-000004.tar', 15), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...in center in windows server 2019', 'the galaxy j7, as well as the full screen, is shown on the table', 'the screenshot showing the credit card reconciliation and the location of the payment options'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00007550', __restore_key__=('MapDataset', 79, 'Webdataset', 'pretrain-000004.tar', 16), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...desert', 'a large disney minnie mouse statue sitting in a chair with two flowers', 'a close up shot of the flower of a pink and yellow lantana', 'lincoln cathedral in lincoln, lincolnshire, england'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004354', __restore_key__=('MapDataset', 80, 'Webdataset', 'pretrain-000004.tar', 17), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ords magnesium and carbon', 'a woman wearing a brown zebra print blouse', 'a gray paper texture with white spots stock photo', 'floor plans of homes in stonecrest for small home designs and layouts'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00021602', __restore_key__=('MapDataset', 81, 'Webdataset', 'pretrain-000002.tar', 26), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... niro with its large hood on', "happy saint patrick's day specials", 'the graph that shows the temperature at scudby weather trends for estcaugs in november', 'install visual studio code on windows'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00016957', __restore_key__=('MapDataset', 82, 'Webdataset', 'pretrain-000003.tar', 8), __subflavor__=None, __subflavors__={}, images=[<PIL.Image....adjustable wire', 'professional ultrasonics with 3 headpieces', 'yellow and white cubic - cut sunflower charm', 'a gym sign on a black background rectangular sticker', "bailey's irish cream liqueur"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00015328', __restore_key__=('MapDataset', 83, 'Webdataset', 'pretrain-000001.tar', 14), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... book with the words brilliant writing tips for students', 'a gold plated plaque depicting the motto of district court of appeal', 'wahoo speed meter with a white, black, and silver logo on the top'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00013284', __restore_key__=('MapDataset', 84, 'Webdataset', 'pretrain-000001.tar', 15), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...e of puzzle icon with a metallic frame', 'a cute bear with a ball and a basket of sport balls a set of illustrations of a young bears character', 'the map of brunei showing cities and capital areas'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00014338', __restore_key__=('MapDataset', 85, 'Webdataset', 'pretrain-000000.tar', 15), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ng card', 'an angel silhouette on the back of an iphone case covers for iphone 4', 'vitamin express alpha lipoic acid with b - 12mg', 'a red wedding gown with gold appliques and sequins on the bust'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00008178', __restore_key__=('MapDataset', 86, 'Webdataset', 'pretrain-000003.tar', 9), __subflavor__=None, __subflavors__={}, images=[<PIL.Image.... metal buckle', 'the 2006 chevrolet avalanche is left outside a dealers garage', 'a man in a hat fishing in the river with a rod', 'the weekly wsu writing assistance report for parents / guardianss'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00012129', __restore_key__=('MapDataset', 87, 'Webdataset', 'pretrain-000003.tar', 10), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...irt regular fit by danielle wright", 'a heart made up a lot of diamond stones on a dark background royalty illustration', 'econ lodge', 'yugio dark crisis trading card game, booster booster, sealed'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00003048', __restore_key__=('MapDataset', 88, 'Webdataset', 'pretrain-000004.tar', 18), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ign with', 'esl teams on stage at the cologne gaming festival', 'a woman gardener is shown in the garden with flowers surrounding her', 'a house with a tan siding and some red bushes in front of it'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00004011', __restore_key__=('MapDataset', 89, 'Webdataset', 'pretrain-000002.tar', 27), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...hly rated by our customers'reviews on auto trader 2019", 'white flower poster by bill powers', 'b & n air filter restore kit', 'the golden autumn arrangement in port orange on, cape florida florist'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00018580', __restore_key__=('MapDataset', 90, 'Webdataset', 'pretrain-000002.tar', 28), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...t at the premiere of the new york story held at grauch pavilion in', 'an image of some pinboards with some items', 'two red carriage doors, in front of a house', 'skreinered ethically custom shirts'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00016026', __restore_key__=('MapDataset', 91, 'Webdataset', 'pretrain-000004.tar', 19), __subflavor__=None, __subflavors__={}, images=[<PIL.Image... the frame with a name and a family pendant', 'the dmq audio processor has been installed with usb outputs', 'a very sexy, plump, and wavy lady', 'keep calm and party with a hockey player postcards'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00023873', __restore_key__=('MapDataset', 92, 'Webdataset', 'pretrain-000000.tar', 16), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ke pie', 'the winter fancy food show hosted by sfa', 'the service cloud service diagram', 'civilization game with different faces and different locations', 'a diy fall art project with a kids hands'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00011678', __restore_key__=('MapDataset', 93, 'Webdataset', 'pretrain-000003.tar', 11), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...ked on top of each other', 'a map of the castle at reims cathedral', 'an analytics screen showing an image of gold prices', 'dragon age inquisition xbox one', "women's elegant ankle length trousers"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00002821', __restore_key__=('MapDataset', 94, 'Webdataset', 'pretrain-000000.tar', 17), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...with white bean sauce, chilis and garlic', 'cooler master 750 - acc aca49seg6 650w power supply with s9 & s', 'a metal bike that is on top of a shelf', 'paul smith autumn 2011 menswear fashion show'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00023172', __restore_key__=('MapDataset', 95, 'Webdataset', 'pretrain-000001.tar', 16), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...oman joggers along the beach stock video footage', 'a red vodafone sign hanging off the side of a building', 'just for men mustache & beard, ash brown', 'the sea thy melody ensemble music of tagore'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00005470', __restore_key__=('MapDataset', 96, 'Webdataset', 'pretrain-000002.tar', 29), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...with the image of the palace of quetz', 'the new renault micra turbo concept', 'a close up of a minifig with blue hair and a hat', 'fuel truck with tank on the road against the blue sky stock photo'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00004907', __restore_key__=('MapDataset', 97, 'Webdataset', 'pretrain-000000.tar', 18), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...edinburgh', 'the new microsoft lumia phone', 'swift berry family font', 'a christmas holiday flyers pack', 'a grey buick car parked inside of a garage', '20 fascinating ideas for spring porch decor'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00014207', __restore_key__=('MapDataset', 98, 'Webdataset', 'pretrain-000001.tar', 17), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...while departing at lax", 'double happiness, chinese calligraphy, prosperity, double - sided acryle key chains', 'olay natural white instant', 'prince harry and wife meg obama at a paralympics event'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00004606', __restore_key__=('MapDataset', 99, 'Webdataset', 'pretrain-000001.tar', 18), __subflavor__=None, __subflavors__={}, images=[<PIL.Image...se student success high school to mba', 'the two men are practicing judo and jika', '2056 6th street, northridge, ca 91923 sb16967904 re', 'a mother tiger and her two babies playing with each other'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00010019', __restore_key__=('MapDataset', 100, 'Webdataset', 'pretrain-000001.tar', 19), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ed petticoat', 'what is net present value analysis', 'character traits of high self - esteem', 'nutrition facts label with a woman', 'the hotel pulitzer at night', 'insurgency - call of duty sniper'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00010372', __restore_key__=('MapDataset', 101, 'Webdataset', 'pretrain-000003.tar', 12), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...and sun protecting gel for the face', 'a calendar with australia holidays 2021 and 2020', 'medical assistant cover letter', 'the mormon mormons temple, salt palace', 'aria accent chest with drawers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00010068', __restore_key__=('MapDataset', 102, 'Webdataset', 'pretrain-000002.tar', 30), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 'a painting of a bluegill fish', 'the dress is a floral print with a halter neckline, asymmetric hem and a draped,', 'pawn pawn player on board,', 'a ladybug necklace with a pendant charm attached'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00019134', __restore_key__=('MapDataset', 103, 'Webdataset', 'pretrain-000000.tar', 19), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...l travel packages on it', 'soccer player in australia colours with the flag of australia and a football kicking boot photo', 'the book and cover of blues power, the life and music of paul rosenberg'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00001905', __restore_key__=('MapDataset', 104, 'Webdataset', 'pretrain-000000.tar', 20), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...untryvan parked on the side of a building', 'a variety of pet food tins, variety pack 12 cans', "alabama quarterbacks michael walker 13 and jimmy gartz 7 during the razor's college football game in"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00013379', __restore_key__=('MapDataset', 105, 'Webdataset', 'pretrain-000003.tar', 13), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...sponge flee zip - up hoody by adamra", "wall decal featuring a hand drawn message saying don't let the muggles get you down", 'the bathtub company 71 in l x 32 in w x 19 in h polyure quartz acrylic'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00010801', __restore_key__=('MapDataset', 106, 'Webdataset', 'pretrain-000002.tar', 31), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ttons', 'the five illustrations in a swedish folk painting depicting various children dressed in traditional clothes', 'a pressure gauge, pressure gauge with dial, barometer with readings, close up'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00017897', __restore_key__=('MapDataset', 107, 'Webdataset', 'pretrain-000004.tar', 20), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ment case with an adjustable door', 'baby girls lace bow soft soled first walkers baby shoes', "women's embroidered grey party wear saree", 'an old red bicycle leaning against a green wood mousepad'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00013983', __restore_key__=('MapDataset', 108, 'Webdataset', 'pretrain-000000.tar', 21), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...outh and the words treat written above it", 'the letter e icon on white background from people collection an alphabet', 'a bench with cement planters and succulentes', 'a green fox camper lunch set'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003808', __restore_key__=('MapDataset', 109, 'Webdataset', 'pretrain-000001.tar', 20), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...dge on the sea', 'a woman who is doing the hand stand, standing on her knees and her head in the floor, her', 'the tourbillon is a watch that is not too complicated for a watch that is a big, small'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00023278', __restore_key__=('MapDataset', 110, 'Webdataset', 'pretrain-000000.tar', 22), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ith zippers', 'michael kors wyatt booties', 'the software apps app, showing some different types of software, and the software and features of its', '100 icicle lights on a porch and on the ceiling'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00021717', __restore_key__=('MapDataset', 111, 'Webdataset', 'pretrain-000004.tar', 21), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eenshot from mxgp 2012', 'german chancellor angela merkelle', 'mtn and out there media', 'counter strike global offensive logo', 'a small audi v6 engine has a small v3 intake', 'the title for limbo'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00023225', __restore_key__=('MapDataset', 112, 'Webdataset', 'pretrain-000003.tar', 14), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...multiplayer weapons', 'the new jersey devils are trying to sell forward jonathan peters', 'a folder with text, illustrating parent and child welfare', 'the as roma players celebrate after the serie'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00001110', __restore_key__=('MapDataset', 113, 'Webdataset', 'pretrain-000001.tar', 21), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...stack of chocolate mints with a bite taken out of the middle', 'woman putting makeup on another woman, in front of mirror stock images', 'rhino with big horns and large ears at the zoo stock images'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00012062', __restore_key__=('MapDataset', 114, 'Webdataset', 'pretrain-000003.tar', 15), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...undpad', 'a black shirt with the image of person, and his band name on it', 'sabian 20 heavy crash ride cyb20tc', 'final fantasy vii', 'the carbon toy hauler is a great toy for camping in the woods'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00000354', __restore_key__=('MapDataset', 115, 'Webdataset', 'pretrain-000004.tar', 22), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ry loaf with blueberries and cream cheese glaze', 'a restaurant website layout design with the theme of a restaurant and catering website design', 'many people in wizard wigs and harry potter robes'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00002624', __restore_key__=('MapDataset', 116, 'Webdataset', 'pretrain-000001.tar', 22), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... field', 'actress madonna jovovich attending the 70th cannes film festival at palais, france on 19 may 2013', 'the iphone se and the iphone 5s', 'alphabet and number set on white paper sheet vector'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00023254', __restore_key__=('MapDataset', 117, 'Webdataset', 'pretrain-000000.tar', 23), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ld cup glory', 'marc perry iphone 4 case', 'the earth is surrounded by space dust and red stars, elements of this image furnished by nasa - stock photo', 'a car parked outside of a brick restaurant'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000748', __restore_key__=('MapDataset', 118, 'Webdataset', 'pretrain-000000.tar', 24), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...on perform on stage during day one of reading festival 2012 at richard park reading', 'a living room has wood beams and beamed ceilings, while a wrought wrought chandelier hangs above the fireplace'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00011142', __restore_key__=('MapDataset', 119, 'Webdataset', 'pretrain-000004.tar', 23), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...re scripture print for the bible, on a white background', 'willem de hoevet 1659 - 76 oil painting reproduction', 'knight tx camera neutral variable circular uv filter 49mm 77mm 67mm 72mm 62mm 78mm'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00018831', __restore_key__=('MapDataset', 120, 'Webdataset', 'pretrain-000002.tar', 32), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...monday, june 29, 2012', 'two people shaking hands over an office binder with the word mou on it', 'the 2008 mercedes - benz e350', 'the man who played jono, who was a character from game of thrones'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00022123', __restore_key__=('MapDataset', 121, 'Webdataset', 'pretrain-000001.tar', 23), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...pen beta 17 9 on oneplus 5 and oneplus 57', 'the hottest dressed celebrity stars at the oscars', 'the resume and job description screen on a smartphone', 'the weather trend for new caledonia in may'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00006907', __restore_key__=('MapDataset', 122, 'Webdataset', 'pretrain-000004.tar', 24), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nit - based safety program uscp was designed to improve', 'the certificate of participation template is also available as pdf format and has a picture of a person on it', 'the green party manifesto'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00010437', __restore_key__=('MapDataset', 123, 'Webdataset', 'pretrain-000003.tar', 16), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... football club', 'adidas nm nmd runner core shoes black uk7', 'the corrections officer exam', "the cowgirl sweater company men's cow herder 1 / 4 zip sweater", 'a blue world map powerpoint template'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003371', __restore_key__=('MapDataset', 124, 'Webdataset', 'pretrain-000001.tar', 24), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rer poses in a dark green swimsuit', 'a white mercedes maybach with a police car in the background', 'a mobile phone power bank with a keychain', "coach john shanton yelling out for his team's loss"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017349', __restore_key__=('MapDataset', 125, 'Webdataset', 'pretrain-000001.tar', 25), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...pany', 'the poster for the siena fair, italy postcards', 'the moto g7 with leopard pattern cover', 'samsung split air conditioner for sale', 'three men are working with the first aidt and an infant'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00007384', __restore_key__=('MapDataset', 126, 'Webdataset', 'pretrain-000004.tar', 25), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... a neckline', 'a chart from the huffington blog on the new york times', "a bib with the words, these pool's puppy cape on backwardss", 'a scale with a balancer, and the words robotic surgery center'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00006918', __restore_key__=('MapDataset', 127, 'Webdataset', 'pretrain-000003.tar', 17), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...s', 'the cover of ghost world', 'an old fashioned steam engine sitting on display at a showgrounds in maine duvet cover', 'vector 3d illustration set of stadium, football arena and field in isotere'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00011891', __restore_key__=('MapDataset', 128, 'Webdataset', 'pretrain-000003.tar', 18), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eave bundles', 'tory boot', 'a stack of whole wheat waffles on a plate with blueberries and raspberries', 'the cast of the parent sitcom saved by the fox network', 'navy metallic turtle neck jumper'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009876', __restore_key__=('MapDataset', 129, 'Webdataset', 'pretrain-000000.tar', 25), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...omponent diagrams', "women's signature jeans, classic fit", 'fire icons set of fire symbols vector', 'a screenshot of the configuration manager interface', 'converse chuck taylor high top red white'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00019630', __restore_key__=('MapDataset', 130, 'Webdataset', 'pretrain-000004.tar', 26), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... in rocket racer 3d', 'a screenshot of the call of duty game with the grenades', 'sonia gandhi in a file photo taken from the youtube video', 'the two competitors are the two main names for wwe raw'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00012393', __restore_key__=('MapDataset', 131, 'Webdataset', 'pretrain-000000.tar', 26), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ont of a microphone vector', 'skyla maker starter pack with skyla', 'weather worksheet', 'a man sits on a chair with a remote control and drinking coffee in the living room vector illustration in a'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00008792', __restore_key__=('MapDataset', 132, 'Webdataset', 'pretrain-000003.tar', 19), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...'oxford storyland readers - the space museum', 'alphabet tracing kit for pre - k, pre - k and preschool', 'the hotel at tajwa, in mumbai', "women's georgette party wear saree", 'uag wrap sunglasses'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00023306', __restore_key__=('MapDataset', 133, 'Webdataset', 'pretrain-000003.tar', 20), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...reen shot of the apple's security settings", 'the video game fifa pro will soon be released on android', 'a newswoman with glasses and a suit', "the cash statement of a company's quarterly earnings"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00010100', __restore_key__=('MapDataset', 134, 'Webdataset', 'pretrain-000004.tar', 27), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rstar womens', 'ford transit custom in a parking lot', 'a red plymouth valiant with a silver stripe', 'the raker framed print by olivier bourret', 'the book america the poisoned by lewis regenstein'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009945', __restore_key__=('MapDataset', 135, 'Webdataset', 'pretrain-000000.tar', 27), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ing on canvas', 'federal small rifle match primed brass rims', 'a map of the city of lolo', 'a slide displaying reading activities', "group photo, girls generation, japan, girls'generation, fashion"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00004408', __restore_key__=('MapDataset', 136, 'Webdataset', 'pretrain-000000.tar', 28), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rror on the facade of it - stock photo', 'a white toyota highland vehicle parked in front of a mountain', 'dates hanging from a date palm tree against a blue sky', 'a popcorn and film strips vector'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003027', __restore_key__=('MapDataset', 137, 'Webdataset', 'pretrain-000001.tar', 26), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...th ornaments, greenery and glassware', "girls'life application study bible, nlt, pink / multicolor", 'hot cidl with lemon and cinnamon in glass', 'audi rs6 avant is the ultimate road - legal estate'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00021565', __restore_key__=('MapDataset', 138, 'Webdataset', 'pretrain-000000.tar', 29), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... on a tree', 'the common core math worksheet has been designed to help students understand what the missing numbers are', 'the action version of realm lords', 'screenshots of the injustice universe'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00018645', __restore_key__=('MapDataset', 139, 'Webdataset', 'pretrain-000003.tar', 21), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...tar, is not a fan", 'queen sugar', 'michigan state fans cheer after michigan beats indiana 72 - 51 in the ncaa game at carver stadium on october 20', 'six hearts with countries - symbols decorative'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00008683', __restore_key__=('MapDataset', 140, 'Webdataset', 'pretrain-000001.tar', 27), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... era nba white blue 39th cap', 'the angler fish in true blood is terrifying', 'the blackhawks have unveiled a new blackhawks logo and hat', '16 inch toyota avalone wheel 2003 - 2008 oem replacement'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00019627', __restore_key__=('MapDataset', 141, 'Webdataset', 'pretrain-000001.tar', 28), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...sales superstar', "jack nicholson in the original puppetry series's production of the goo goo", 'an app to help you sleep and maintain the sleep cycle', 'a white volkswagen tiro, parked in a garage'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00023582', __restore_key__=('MapDataset', 142, 'Webdataset', 'pretrain-000001.tar', 29), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...sk with mirrors, lipsticks and makeup', 'a dvd cover for the us championship wrestling game', 'the ukulele chords chart for beginners', 'the best of house and electro music vol 2 by various artists'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00022793', __restore_key__=('MapDataset', 143, 'Webdataset', 'pretrain-000004.tar', 28), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...fe hero?', 'a cartoon image of a mario in a construction outfit', 'a gold hair tie is worn over a black and white geometric print headband', 'a grocery basket with shopping supplies in it on a cart'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00017938', __restore_key__=('MapDataset', 144, 'Webdataset', 'pretrain-000002.tar', 33), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...pouff on a white table', 'a beautiful maple leaf with pink and orange gradients t - shirt', 'the robot is shown with the juicer attachments', 'children school backpack my little pony purple unicorn'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00006010', __restore_key__=('MapDataset', 145, 'Webdataset', 'pretrain-000001.tar', 30), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... an olympic games, and the', 'model in red silk cami dress and bare leg ankle', '5123 harvest hill lane, manassa, va 22520 - vakv193034', 'a blackmagic unified extension kit for the blackmagic mini'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00013859', __restore_key__=('MapDataset', 146, 'Webdataset', 'pretrain-000001.tar', 31), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...child baby thick outer", 'love seamless pattern with valentine symbols', 'gma profile screen with google keypad', 'a red reclining chaise', 'genuine windows 10 pro professional 32 64bit license key'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00006600', __restore_key__=('MapDataset', 147, 'Webdataset', 'pretrain-000000.tar', 30), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...mal statues at the national library, washington in 2009', 'a skier traverses the steep slope near mount mckinley, with clouds billowing over them', 'protons have an atomic number , , ,  and  '])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000605', __restore_key__=('MapDataset', 148, 'Webdataset', 'pretrain-000000.tar', 31), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...3d rainbow wave', 'santa claus poses for a family photo at the santas christmas tree farm', 'the large dial of the large black dial of the armogann man watches is decorated with green detailing and'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00020594', __restore_key__=('MapDataset', 149, 'Webdataset', 'pretrain-000004.tar', 29), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...th rear wheel and tyres', 'a wood figure', 'a bracelet with a leather clasp and green leather, and a gold hook closure', 'the flexair womens shoe is wearing a black dressier shoes with gold accents'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00002058', __restore_key__=('MapDataset', 150, 'Webdataset', 'pretrain-000004.tar', 30), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ball shoes', 'the beach club condos on the beach', 'a wedding party at the boathouse on the river tweed in berwick, scotland', 'the terraces at pisan, peru', 'a computer engineering resume template'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00022399', __restore_key__=('MapDataset', 151, 'Webdataset', 'pretrain-000001.tar', 32), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... "the disneyland resort's new water park is coming to the disney resort in november", 'a young man with two axes, with a horror mask on top of his head', 'a 2020 jayco motorhome in the fall foliage'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00011851', __restore_key__=('MapDataset', 152, 'Webdataset', 'pretrain-000001.tar', 33), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nterior', 'a vase of stargazer lilies in scranton pa, john abraham floral gallery', 'the deloreman back to the future car', 'the corvette chronicle magazine', 'theia one - shoulder drape maxi dress'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00007919', __restore_key__=('MapDataset', 153, 'Webdataset', 'pretrain-000000.tar', 32), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nterior design with wood accents and white walls', 'the empty music room in the front row of the studio, where the music classes are', 'a bedroom with blue walls and bed with gold decor on the side'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00011379', __restore_key__=('MapDataset', 154, 'Webdataset', 'pretrain-000002.tar', 34), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ng installation service', 'estero advanced night repair, 1 fl oz', 'superhero printable party pennants for birthday parties', 'the marble kitchen island in this home is a bright and cheerful choice'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009468', __restore_key__=('MapDataset', 155, 'Webdataset', 'pretrain-000000.tar', 33), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...oncert with a band of musicians and singing, on stage', 'a young boy and girl with books vector', 'quickbooks online qbo tutor - new asset purchases', 'tracy morgan starring twice netflix billboard'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00013499', __restore_key__=('MapDataset', 156, 'Webdataset', 'pretrain-000003.tar', 22), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... side and the side handles and straps on the outside', 'fuel injector for toyota oem 3710 - 24770', 'a cat brooch ornament on a white background', 'the paris postcard on the white background vector'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00005170', __restore_key__=('MapDataset', 157, 'Webdataset', 'pretrain-000000.tar', 34), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...h side plea', "valentine's day worksheets for preschool", 'the fur clutch in beige and black', 'four trees in the fog poster', 'a novel written by the astronaut wives club', 'a red fire escape door'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00018182', __restore_key__=('MapDataset', 158, 'Webdataset', 'pretrain-000004.tar', 31), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ompetes in the audi fisa world cup race at hoceldbach', 'tea plantations are located on the steep slopes near the village of havan', 'the 2016 buick lacrosse is driving down a paved road near water'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00019074', __restore_key__=('MapDataset', 159, 'Webdataset', 'pretrain-000001.tar', 34), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ters weekend concert in march 2018', "a woman with a guitar, with the words let's play guitar power chords lesson 8", 'roger and holly madison are pictured in this handout image from playboy weekly'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00016989', __restore_key__=('MapDataset', 160, 'Webdataset', 'pretrain-000000.tar', 35), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... party dress', 'dreams gloss nourishing scalp conditioning lotion', 'the remixes by tom cochrane', 'a white, poly tubing pipe for a large format', 'a grey tile mosaic is seen in this tile adornment'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00015788', __restore_key__=('MapDataset', 161, 'Webdataset', 'pretrain-000000.tar', 36), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...o a bag', "men's minnesota rocket t - shirt, navy", 'the word gorilla on top of a green leaf', 'nylon zipper with a yellow edge and two gold zips', 'a girl in a knight suit with crows round sticker'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007955', __restore_key__=('MapDataset', 162, 'Webdataset', 'pretrain-000003.tar', 23), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ggenheim museum', 'two uncooked raw chickenes on a paper bag in the kitchen', 'new 2020 porsche macla suv', 'a blue 2019 jeep cherokee suv', 'autumn aspens in fall greeting card by panoramic images'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00016177', __restore_key__=('MapDataset', 163, 'Webdataset', 'pretrain-000002.tar', 35), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...s aprons', 'the candy land game is made of cardboard with a piece of plastic', 'a six - tier black and gold wooden jewelry organizer', 'mankacare portable patient radiology monitor for hospital use'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00001016', __restore_key__=('MapDataset', 164, 'Webdataset', 'pretrain-000000.tar', 37), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... his visit to the la dodgers baseball museum in 2014', 'a pembroken dog with tongue open in the park a pet is sitting on the ground stock photos', 'a man drawing an enterprise diagram on the screen'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00008658', __restore_key__=('MapDataset', 165, 'Webdataset', 'pretrain-000001.tar', 35), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rown and beige collar', 'bath soap in a white bottle with a flower on it', 'new 6 pieces mini toys plastic animal puzzle toy children learning educational toys, educational toys toys china mainland'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00002003', __restore_key__=('MapDataset', 166, 'Webdataset', 'pretrain-000002.tar', 36), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'lionel verraza left with a leg pain on the ground as the argentina team stretches during a training session', 'the eiffel tower book cover', 'the music artist of jimi hendrix poster by agedrikan'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004091', __restore_key__=('MapDataset', 167, 'Webdataset', 'pretrain-000004.tar', 32), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...s images and quotes', 'a man in a balard holds a pick axe', 'patio and deck by a custom pool in new jersey', 'handmade elephant mask in kindergarten classroom', 'gold foil pineapple cushion, 18 x18'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00014784', __restore_key__=('MapDataset', 168, 'Webdataset', 'pretrain-000003.tar', 24), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...logo on the left chest", 'a coach wristlet in leather with an external zipper in a colour cream', 'a fine - art nouveau style painting depicting two women wearing white clothes with long red skirts'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00010492', __restore_key__=('MapDataset', 169, 'Webdataset', 'pretrain-000003.tar', 25), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...th christmas decorations and wreaths with the words advent bible journal devotional', 'spider and web worksheet', 'the nike trink waist pouch', 'employment policies for creating decent work for all'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00018286', __restore_key__=('MapDataset', 170, 'Webdataset', 'pretrain-000000.tar', 38), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...orum 1977 beetle gtx wiring diagram', 'fulton, fanny - lucas county, iowa | fanny a picture of fulton - iowa gravestone photos', 'people on nanjing street, a city that is famous for its attractions'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00019714', __restore_key__=('MapDataset', 171, 'Webdataset', 'pretrain-000001.tar', 36), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...snowy field stock video footage', 'the history logo with the letters h on top of it', 'a screen showing the game progress button in riot squad', 'the runway show from the london fashion week runway'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00008885', __restore_key__=('MapDataset', 172, 'Webdataset', 'pretrain-000004.tar', 33), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ke decoration is on top of the cake', 'a pilatesh demonstrating her cores in a reformer', 'a backpack and toilet roll bag on a white background', 'the indigo stripes back cover for motorola moto g5'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00019687', __restore_key__=('MapDataset', 173, 'Webdataset', 'pretrain-000003.tar', 26), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...round the world', 'tony abbott and bob crow in parliament chambers', 'arduino relay with high voltage devices and a lamp', 'installing a wiring harness for the door handle and rear window - youtube'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00023326', __restore_key__=('MapDataset', 174, 'Webdataset', 'pretrain-000001.tar', 37), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...cura sedan driving down a road', 'the female version of the game avatar in guildia', 'a red ferrari enzo gt is parked in front of a house', 'a pendulum balls and pendulums with dark grey background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00000615', __restore_key__=('MapDataset', 175, 'Webdataset', 'pretrain-000002.tar', 37), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...om the upper deck of the mercedes superdome', 'debt consolidation and the consequences of debt loss', 'living room - 42324 river shore view ct, ashburn', "the redblaaver girls by michael o'sullivan"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00019485', __restore_key__=('MapDataset', 176, 'Webdataset', 'pretrain-000000.tar', 39), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ff his spinach plant with watercolor pencils', 'the different designs in the bird and birdie character pack', 'a man is doing a fire show in the night time', 'the hyundai elantra offer for $ 8, 899'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00023167', __restore_key__=('MapDataset', 177, 'Webdataset', 'pretrain-000003.tar', 27), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...with the battle', 'a bowl of hamburger soup with text that says delicious hamburger soup', 'the flag of the indian republic with text overlays', 'a large minecraft farm surrounded by a big building'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00006554', __restore_key__=('MapDataset', 178, 'Webdataset', 'pretrain-000002.tar', 38), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...f the bookgraphic user interface design and evaluation by david edmond - pile', 'a very textured surface of cement stock photo', 'a movie poster for the ufc team', "nike men's running vest - orange"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00006699', __restore_key__=('MapDataset', 179, 'Webdataset', 'pretrain-000002.tar', 39), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...society pre - oscar party', 'blue jordan retro shoes on feet', 'a foot in the door book cover', 'the letterpress stationery mock', 'how to glue buttons on paper macht', 'the royal wedding programme'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00018506', __restore_key__=('MapDataset', 180, 'Webdataset', 'pretrain-000002.tar', 40), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ngs', 'a woman holds up an issue of the flash comic', 'a cartoon shoes with a heart in a blue shoe with white soles', 'the new issue of o magazine and oprah', 'the king of spades poker playing card'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007822', __restore_key__=('MapDataset', 181, 'Webdataset', 'pretrain-000003.tar', 28), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ment entitled'a party ', and it is called a play party", 'prince william and prince edward at remembrance event', 'chinese food machines and machinery - china hot sale commercial flattening machine'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017320', __restore_key__=('MapDataset', 182, 'Webdataset', 'pretrain-000000.tar', 40), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...er for women by thierry hen', 'a skateboard with the word iowa on it', 'the nightmare sticker set by fun express', "the patix women's insulated shelling pants", 'plaster plaster, small, light beige'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00020264', __restore_key__=('MapDataset', 183, 'Webdataset', 'pretrain-000004.tar', 34), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... collection of clothing designs for women's wear", 'several people hold up facebook symbols in their hands with their hands in the air', 'pub player standing in the middle of a street while playing'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00001476', __restore_key__=('MapDataset', 184, 'Webdataset', 'pretrain-000000.tar', 41), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 'the helicopter parked next to a circus tent at the philadelphia convention center', '2 bedroom detached house to rent in oakwood, ashercombe', 'the small town of ostuna, in the province of sicily'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00020252', __restore_key__=('MapDataset', 185, 'Webdataset', 'pretrain-000000.tar', 42), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... posing for a picture with other pussy ladies', 'jennifer loefst small stone bangle', 'family rafting on the kicking river', 'the new winter collection stickers', '7 wardrobe pieces worth spring on'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00019039', __restore_key__=('MapDataset', 186, 'Webdataset', 'pretrain-000004.tar', 35), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... zuo', "a fingerprint image of two people's fingers", 'bradley fernandez and penelope jete attend the presentation of their film, i am legend', 'people wearing panda masks are holding up palm trees'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00004059', __restore_key__=('MapDataset', 187, 'Webdataset', 'pretrain-000002.tar', 41), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e, a luxury cabin in a secluded lake near bolton, bedfordshire', 'jack fruit, big fresh exotic fruit', 'a small toy bunny sitting on a table near many books', 'galaxy j3 2017 smr leather cover gold'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00011955', __restore_key__=('MapDataset', 188, 'Webdataset', 'pretrain-000004.tar', 36), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...smiles for the camera with boke lights in the background', 'dw home breathe essential fragrance candle in eucalyptus and clary mint', 'the hike route from heidelberg to schertz in the austrian alps'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00016388', __restore_key__=('MapDataset', 189, 'Webdataset', 'pretrain-000004.tar', 37), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...w to enroll a new mexico school nurse', 'retro sunburst background in 4 variations illustration', 'an adjustable height chair with grey leather upholstered seat', 'therion - broken hearts album mp3'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017488', __restore_key__=('MapDataset', 190, 'Webdataset', 'pretrain-000001.tar', 38), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nd sweets', 'an suv tyre on a white background', 'the sun throw pillow by jeff young', 'the royal canadian air force coffee mug', 'the bride and groom are shown in their batman wedding cake toppers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00001999', __restore_key__=('MapDataset', 191, 'Webdataset', 'pretrain-000001.tar', 39), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ar race fans crowding around a racing track during a daytona 400', 'the mid - states poker tour is in jeopardy on the way to participate at mspt', 'a house with a side view of the roof and fasciats'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017403', __restore_key__=('MapDataset', 192, 'Webdataset', 'pretrain-000000.tar', 43), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ile phone and laptop', 'the horse head planter in bronze', 'vets best dental care gel for dogs', 'the dead and buried a novel of suspense by stephen booth', 'western horse show saddle with cow horn'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00010499', __restore_key__=('MapDataset', 193, 'Webdataset', 'pretrain-000004.tar', 38), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ssue of suburban good housekeeping', 'the frankie animal print blouse', 'diagram of process flow flow from analysis', 'pub rules on mobile', 'nursery sticker for wall of stars', 'galne netting roll'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00010719', __restore_key__=('MapDataset', 194, 'Webdataset', 'pretrain-000002.tar', 42), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ey role of the commodity exchange in the gas market', 'a game of casino slot with various cash piles', 'the cottage at westcliffe', 'lapis lazula and silver chain necklace with blue jasperite beads'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00010944', __restore_key__=('MapDataset', 195, 'Webdataset', 'pretrain-000004.tar', 39), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... canvas print featuring the photograph of a lion by person', 'ketchup bottles in a grocery store', 'the cover of nfl magazine, with broncos player trevor van laem', 'paylip email with employee name'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00009053', __restore_key__=('MapDataset', 196, 'Webdataset', 'pretrain-000002.tar', 43), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...a dirt road', "the cavaliers'game is being made available for the web", 'attractive pink and green colored georgette party wear saree', 'the 2013 ford taurus', 'a painting of a dog on a paint field'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007208', __restore_key__=('MapDataset', 197, 'Webdataset', 'pretrain-000003.tar', 29), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ture', 'disney characters as girls disney princess drawings, disney love, person, disney art, disney films, disney princess', 'an image of the top side of a knitting scarf with the hood pulled back'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00002781', __restore_key__=('MapDataset', 198, 'Webdataset', 'pretrain-000000.tar', 44), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...uildings', 'the brandenburg monument berlin in germany watercolor on paper', 'the bodyguard was spotted holding hands with kim, and her ex - husband', 'the cliffs in acadiah, maines upper peninsula'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00008944', __restore_key__=('MapDataset', 199, 'Webdataset', 'pretrain-000001.tar', 40), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... sheep stands in a field of green grass with dandelions stock images', 'a silver cat ring with black eyes on top', 'copper pipe, copper fittings, and copper connectors', 'harry potter pop - up book'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00021179', __restore_key__=('MapDataset', 200, 'Webdataset', 'pretrain-000002.tar', 44), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eveling interface, showing the creation process', 'the protagonist in the second - person mode in resident district, using different lighting sources', 'reps question the situation on capitol floor'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003401', __restore_key__=('MapDataset', 201, 'Webdataset', 'pretrain-000001.tar', 41), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...r by jack & jones - blue, orange, stripes, buttons, casual, fall', 'people at the kaaba in mecca mosque', 'large quantity of leather masks in a market', 'the brandenburg brandenburger tor at sunset'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017525', __restore_key__=('MapDataset', 202, 'Webdataset', 'pretrain-000000.tar', 45), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...les cover art of the day', 'the pinky and the beast logo', 'house of the dragon game of thrones logo', 'a mobile height adjustable over bed table with wheels', 'a beige heeled shoe from jimmy jimmy'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00022315', __restore_key__=('MapDataset', 203, 'Webdataset', 'pretrain-000004.tar', 40), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...oollers", 'the two tigers, pictured from the tiger king, have been separated since this photo is released on the internet', "michael o'lough on the red carpet at the london premiere of the hittites"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00023455', __restore_key__=('MapDataset', 204, 'Webdataset', 'pretrain-000004.tar', 41), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...n a displayroom', 'wolverhampton fc - reading v wolves', 'an asian woman sells corn at a market in yangyang, anhui province june 24, 2009 reuters /', 'an op circuit with a resist resist and current'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00023375', __restore_key__=('MapDataset', 205, 'Webdataset', 'pretrain-000001.tar', 42), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... images show the kids on snow tubing', 'free people paisley pants with plea', 'a hand shake with a flag behind it', 'a home page of a property management website', 'a silver mercedes camper is seen'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00008894', __restore_key__=('MapDataset', 206, 'Webdataset', 'pretrain-000004.tar', 42), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...and white boxes', 'a photo of the baby polar bear learning area', 'a nokia phone, showing the weather display', 'a scene from comedy serial sahil khan', 'the back of the davine washing machine door'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00015727', __restore_key__=('MapDataset', 207, 'Webdataset', 'pretrain-000001.tar', 43), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...t says erie canal 1829', 'an rna structure and the molecule on top of it', 'nike jerseys for men denver broncos nike football jerseys', 'contemporary led 3 - light floor lamp with black glass shade'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002630', __restore_key__=('MapDataset', 208, 'Webdataset', 'pretrain-000003.tar', 30), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ichael's auto sales in fort lauderdale, florida", 'a colorful rainbow eye is on a black background', 'a photo of the author of the growth to freedom podcast', 'a grey 2020 bmw m85 driving on a road'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000326', __restore_key__=('MapDataset', 209, 'Webdataset', 'pretrain-000000.tar', 46), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...tion during the odi cricket tournament', 'the vatican army museum in rome, italy', 'a pug dog and two ukuleys, one is sitting next to each other', 'the beach in el nido national park, puerto puerto'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00020920', __restore_key__=('MapDataset', 210, 'Webdataset', 'pretrain-000003.tar', 31), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...anemon', 'a group of business people working at computers stock photo', 'a poem from john locke in an altered image with the words we love you', '99 no deposit casino bonus at landmark bingo casino'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00006445', __restore_key__=('MapDataset', 211, 'Webdataset', 'pretrain-000000.tar', 47), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...l', 'the lifeguard stand is equipped with the water supply station, while a child plays in the foreground', 'the dragon ball film page 3', 'two large storage closets that are under a sloped ceiling'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00010050', __restore_key__=('MapDataset', 212, 'Webdataset', 'pretrain-000001.tar', 44), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...or coffee ceremony', 'how to change default account in windows xp', 'home design floor plans youtube on house plan creator', 'camelbak hydrar pack in neon yellow', 'darting whiskey glass set of two'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00001187', __restore_key__=('MapDataset', 213, 'Webdataset', 'pretrain-000003.tar', 32), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...son', 'the bristol suspension bridge crossing over the river severn in bristol', 'a lot of stone wall background, old bricks wall stock images', 'sandwiches and pastries on display in a bakery case'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00011721', __restore_key__=('MapDataset', 214, 'Webdataset', 'pretrain-000002.tar', 45), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...tival', 'the jewelry of the cross bracelet', 'a roll of pty stretch tape', "baby girl's patent leather mary jane shoes with bow", 'an example of home design software for house construction software'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00022334', __restore_key__=('MapDataset', 215, 'Webdataset', 'pretrain-000001.tar', 45), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e case', 'nba mockups for 2014 - 19 college basketball season', 'a grey 2020 mazda cx - series parked in front of a restaurant', 'a woman is shown with an image of her book, soulless is not a movie'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00016718', __restore_key__=('MapDataset', 216, 'Webdataset', 'pretrain-000003.tar', 33), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 50cc motorcycle', 'wheel polishing kit, p 80mm', 'men standing next to a hay cart full of hay', "a women's watch with brown crocodile leather strap", 'twin over full bunk bed with trundle mattress'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00001246', __restore_key__=('MapDataset', 217, 'Webdataset', 'pretrain-000003.tar', 34), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... with the cover of a book'how i can help my body control overweight '", 'a stack of pencil topper pom poms with rainbow loom bands', 'a colonial style house in a small village in goa pradesh, india'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00005110', __restore_key__=('MapDataset', 218, 'Webdataset', 'pretrain-000004.tar', 43), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...the perfect gourmet hamper', 'bronze metal lettering seize the day with green and yellow grungy background', 'baseball manager 2 for android', '1998 pacific eclipse speed and light red 5 jerry rice'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002146', __restore_key__=('MapDataset', 219, 'Webdataset', 'pretrain-000003.tar', 35), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nge and pink ukulei sitting on a shelf', "pottery pots on display at a potter's shop", 'the frankfurt skyline at night stock photo  phaet', 'a social security certificate with a pen and eyeglasses'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00004669', __restore_key__=('MapDataset', 220, 'Webdataset', 'pretrain-000000.tar', 48), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...', 'two covers for the v magazine', 'demin crawford in a skirt leaving a salon in west hollywood, ca', 'morphic resonance the nature of formative causation', 'sneakers for women from tommy hilfiger'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00016769', __restore_key__=('MapDataset', 221, 'Webdataset', 'pretrain-000001.tar', 46), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...resident george', 'an airplane propeller hanging in a building', 'philosophy - blush and glow compact powder pact', 'pair of sneakers on a table tote bag', 'women running white ultraboost dna shoes'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00007543', __restore_key__=('MapDataset', 222, 'Webdataset', 'pretrain-000001.tar', 47), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... sits on the floor next to a toy duck', 'a child sits in a destroyed area following an airstrike in the rebel town of homs', 'a model wears a cropped top and slited skirt from the resort collection'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00009020', __restore_key__=('MapDataset', 223, 'Webdataset', 'pretrain-000004.tar', 44), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...', 'professional marketing resume samples', 'an older man in blue doing martial tai fu', 'isabel tiered ruffled cotton dress by valentino', 'an icon of a computer wifi network on a desktop computer'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003142', __restore_key__=('MapDataset', 224, 'Webdataset', 'pretrain-000001.tar', 48), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... snow covered ground near lake', 'an empty road winds through the mountains as the sun begins to shine', 'a herstle green backpack, with zippers and a tan strap', 'little star sleep suit - 5 pc set'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003848', __restore_key__=('MapDataset', 225, 'Webdataset', 'pretrain-000001.tar', 49), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eces sitting on leaves with two leaves around the top design relaxation residing garden decoration feasting', 'the original cover for the record, featuring john mayall, eric clapton, and john moyla'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00008856', __restore_key__=('MapDataset', 226, 'Webdataset', 'pretrain-000000.tar', 49), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ner pages for 2018', 'the enchanted land book by a v reddy', 'a button down denim mini skirt with high waist', 'imf2 antibody stain in histological stain', 'the book cover for robbie the wonder dog'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00001096', __restore_key__=('MapDataset', 227, 'Webdataset', 'pretrain-000002.tar', 46), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rson stands in front of his work in the galeries paris on may 30 2010 in paris', 'an illustration shows a woman in an angel dress with wings outstretched on the edge of a cliff', 'arma xpex pc game'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017826', __restore_key__=('MapDataset', 228, 'Webdataset', 'pretrain-000000.tar', 50), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...grass dining chairs in chocolate brown', 'tangerine dream, oasis cd', 'the words code are on a black mousepad', 'a bar and wet bar in a basement home', 'the turtle food in a package and the turtles'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00009208', __restore_key__=('MapDataset', 229, 'Webdataset', 'pretrain-000001.tar', 50), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...es cut into it', 'black printed maxi wrap dress with deep v neck', 'an image of two brass garden hose fittings', 'jamie lee is hugged by her dad after he performed his famous duet on the today show'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009125', __restore_key__=('MapDataset', 230, 'Webdataset', 'pretrain-000000.tar', 51), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e home with private garden in the woods', 'advantages of inorganic materials to manufacturing for mechanical engineering materials', 'the checker scarf with fringe is a red, black and white blanket'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004094', __restore_key__=('MapDataset', 231, 'Webdataset', 'pretrain-000004.tar', 45), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... made from rice leaves', 'a certain type of elevator knight cosplay school girl uniform', 'the intruder by colson whitehead', 'the mini lock bag in pink with silver stud', 'patio sofas with pillows'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00020236', __restore_key__=('MapDataset', 232, 'Webdataset', 'pretrain-000002.tar', 47), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... computer screen with video playing', 'the best buy store on main street in ottawa', 'people on bikes and motorcycles riding on the street footage', 'the nike air monarch', 'orphan black on netflix'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00001633', __restore_key__=('MapDataset', 233, 'Webdataset', 'pretrain-000000.tar', 52), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...aco dip and a wooden spoon on the side', 'the newly married couple stand on the stage and celebrate as their ceremony begins', 'the san francisco bay from the top of mountain over looking the beach'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017301', __restore_key__=('MapDataset', 234, 'Webdataset', 'pretrain-000003.tar', 36), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...'an embroidered unicorn pillow with a paste colored background', "the surfer's journal issue 24", 'a blue adirondack with two chairs and a coffee table', 'black druzy earrings on a white background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00010524', __restore_key__=('MapDataset', 235, 'Webdataset', 'pretrain-000001.tar', 51), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 'the andrews sisters', 'a steel filing cabinet in two drawers', 'ddss sydney logo', 'mercedes gle for sale on lifetime warrant', 'a white carhart plaid cap with a small blue and grey plaid pattern'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00008357', __restore_key__=('MapDataset', 236, 'Webdataset', 'pretrain-000002.tar', 48), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... performance as much as vocabulary / does', "the world's ocean basins are all made up of oceans", 'the debt note of hdfc bank for the bank account', 'the college safety 1011 book by liz a greenberg'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00012541', __restore_key__=('MapDataset', 237, 'Webdataset', 'pretrain-000001.tar', 52), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... rough aged cracked surface and jagged edge in a bold rough chi', 'windshield sun shade for motor trend', 'some unripe fruit are growing on the tree', 'some of the tools from an old climbing device'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00015971', __restore_key__=('MapDataset', 238, 'Webdataset', 'pretrain-000000.tar', 53), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...lications with ibm rational application develop and', 'a close up shot of the fabric in lila blue', "victoria's secret vanilla love body mist", 'kickypock cotton romper with feet in blue bear print'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00021025', __restore_key__=('MapDataset', 239, 'Webdataset', 'pretrain-000004.tar', 46), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rs dance in a large ballroom', 'several motorcycles racing down a road at a speed trial', 'an atlas 2 rocket lifts off from pada earth', 'queens pass gondola overlooking queens lake and queens peak'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00005155', __restore_key__=('MapDataset', 240, 'Webdataset', 'pretrain-000004.tar', 47), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...gn greeting card by paul verrier', 'buddhist flags and himalayas mountains in the distance royalty - free stock photo', 'two 3d renders of a medieval castle building on an isolated white background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00010318', __restore_key__=('MapDataset', 241, 'Webdataset', 'pretrain-000001.tar', 53), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'lana del rey on the cover of rolling magazine', 'coaster bed with storage', 'the samsung galaxy ace', 'a casino card stack with the words learn from the pros', 'employee of the month certificate'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00022816', __restore_key__=('MapDataset', 242, 'Webdataset', 'pretrain-000001.tar', 54), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...r blu ray', 'the led sign on display at the iwc in barcelona', 'a carlton player has his head in his hands', 'a 2020 harley davidson road glide', 'a garbage bin is opened in the middle of the floor'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00019092', __restore_key__=('MapDataset', 243, 'Webdataset', 'pretrain-000003.tar', 37), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ident obama talks with saudi's king abdullah bin mohammed bin talyya", 'super smashbros', 'a dna structure made up of molecules', 'jonas hernandez, jonas fernandez, and nick lacica of one direction'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00020152', __restore_key__=('MapDataset', 244, 'Webdataset', 'pretrain-000003.tar', 38), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nference table', 'ayreil moore at the 22nd annual elle women in television event, beverly wil hotel, beverly hilton hotel', 'the modern teen bedroom has wood and white', 'logo for interippid potash'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017536', __restore_key__=('MapDataset', 245, 'Webdataset', 'pretrain-000001.tar', 55), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...t for the action girl', 'the chesterfield briefcase', 'stainless silver sign with word prive', 'the get - over yourself self help book and other essays', 'a photo of a navy blue mens canvas sneaker'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00001727', __restore_key__=('MapDataset', 246, 'Webdataset', 'pretrain-000003.tar', 39), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...te patterns, feathers and a red feather hat', 'a couple is in love, kissing and looking at each other in the window stock photos', 'an aircraft in the air, with one rotor rotors and one rotors down'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00015452', __restore_key__=('MapDataset', 247, 'Webdataset', 'pretrain-000000.tar', 54), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e circles', 'president donald and kim jong un in front of the american flag', 'a set of four tile pieces showing different shapes and materials', 'aristotle giving his state to his doctors postcard'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00003045', __restore_key__=('MapDataset', 248, 'Webdataset', 'pretrain-000000.tar', 55), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...avy dress with waves', 'the side of the mercedes - benz formula team bus', 'this is a photo of the red lehenga', 'a woman wearing a brown lace shrug vest', 'canapes with smoked salmon and cucumbers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00022775', __restore_key__=('MapDataset', 249, 'Webdataset', 'pretrain-000003.tar', 40), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... all - silver outfit', 'the xiao mi mix smartphone with its rear facing camera', 'the lake, in badia, with the village of karbna in the distance', 'a man in a suit and tie talking with a cnn anchor'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00018437', __restore_key__=('MapDataset', 250, 'Webdataset', 'pretrain-000003.tar', 41), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...arfield cartoon para iphone', 'an exercise guide for the human body using whole - body vibration in physical therapy and sport', "sienna and jason o'connell are enjoying their holiday day in venice"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00013431', __restore_key__=('MapDataset', 251, 'Webdataset', 'pretrain-000001.tar', 56), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ments', 'hands gesture drawing, hand reference, gesture drawing, gesture drawing, body language, finger anatomy, drawing lessons', 'a group of carps swimming in the pond stock illustration 63259195'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00005944', __restore_key__=('MapDataset', 252, 'Webdataset', 'pretrain-000004.tar', 48), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... wishes shayari images for wife', 'womens alligator crocodile skin purse handbag shoulder bag with det lock and wrist strap - red', 'cheap price fabric sofa and love seat sets buy sofa set for sale'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00009122', __restore_key__=('MapDataset', 253, 'Webdataset', 'pretrain-000003.tar', 42), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...d pocket watches', 'a loves long sleeve blouse size 14 plus', 'a booklet on education law and surveys with text on top of the booklet', 'the front end of the jeep renegade trailhawk with light bars'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00011118', __restore_key__=('MapDataset', 254, 'Webdataset', 'pretrain-000001.tar', 57), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... mazda mpv wiring diagram manual', 'pastel basket arrangement in newport, or | petals and blooms', 'bone broth protein with turmeric, 750 g', 'an apple a day poster illustration on white background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00012464', __restore_key__=('MapDataset', 255, 'Webdataset', 'pretrain-000003.tar', 43), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ballet instructor is doing a stretching pose with a student', 'two hand drawn faces in graffiti on a wooden surface handwritten faces face wall building graffiti wooden structure background picture'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00001485', __restore_key__=('MapDataset', 256, 'Webdataset', 'pretrain-000002.tar', 49), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'a black tuxedo suit with three - piece pants', 'a two - story property has a back yard with the windows open to reveal a courtyard area', 'the cover of the book, the united states of appalachian'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017105', __restore_key__=('MapDataset', 257, 'Webdataset', 'pretrain-000003.tar', 44), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...h silhouette bride and groom in tuxedo', 'the text queen of auditors, in blue pins', "the cover for donna summer's bad girls deluxe album", 'a pink ice cream cone on a light pink background notepad'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00000322', __restore_key__=('MapDataset', 258, 'Webdataset', 'pretrain-000001.tar', 58), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... a vaccine and a vial', 'the lake in switzerland, with a boat harbor and mountains in the background', 'home services sign', 'a family in blue with a quote that says a trade action for quality care'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00019971', __restore_key__=('MapDataset', 259, 'Webdataset', 'pretrain-000002.tar', 50), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...inal in may 2011', 'a model train on tracks in a toy town', 'someone is measuring their blood sugar level with an accurate glucose meter', 'the translations app is showing english and spanish words'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00000004', __restore_key__=('MapDataset', 260, 'Webdataset', 'pretrain-000004.tar', 49), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...for a day at the grocery', 'the four different types of olives, kalamatas, and olives', 'the new steps logo with a woman holding a blue arrow in front', 'the best bassline for fishing on the market'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00018014', __restore_key__=('MapDataset', 261, 'Webdataset', 'pretrain-000002.tar', 51), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...es of john cheever '", 'an item item for the leveling, which shows how many items are available', 'a rendering of the completed office tower in the city', 'pine branch background with light effects'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00011312', __restore_key__=('MapDataset', 262, 'Webdataset', 'pretrain-000000.tar', 56), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nelius', 'set of 2 flower appliques with crystal stone center', 'la quinta inn', 'the book cover for fundamentals of case management practice by nancy summers', 'wheeled duffel with wheels by merge'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00002466', __restore_key__=('MapDataset', 263, 'Webdataset', 'pretrain-000002.tar', 52), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...a background of blue sky and', "person from the'90s, wearing a t shirt with the stars of the television sitcom", 'the doctor whovian and his tardish', 'a package of johnson and johnson acuvue oasys'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00011344', __restore_key__=('MapDataset', 264, 'Webdataset', 'pretrain-000003.tar', 45), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ement of flowers in madisonville, tx', 'gold pumpkin on a wooden surface with fall decorations behind it', 'blue occasion hat with organ netting and veil', 'blundshoes, blundshoes, blundshoes brown'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007682', __restore_key__=('MapDataset', 265, 'Webdataset', 'pretrain-000002.tar', 53), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...links tokyo city to its river side, and a train going over it', 'a painting of the old bridge in galway poster by mary stevenson', 'the program is funded by the new york chapter of funding day 2012'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00015711', __restore_key__=('MapDataset', 266, 'Webdataset', 'pretrain-000001.tar', 59), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...erson riding a dog with an gnome costume', 'a round dining table with grey wood and cement base', 'dewwell 20 - volt xr impact driver with hard cover', 'socks with snow dogs and a fair isle pattern'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007046', __restore_key__=('MapDataset', 267, 'Webdataset', 'pretrain-000002.tar', 54), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...metic clinic', 'renault c - kle suv privilege, 2 0 turbo diesel, bluetooth', "the facebook logo is seen at facebook's headquarters in menlo park, california", 'a book cover with a lion symbol on it'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004586', __restore_key__=('MapDataset', 268, 'Webdataset', 'pretrain-000004.tar', 50), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...per, part 6', "men's lightweight flee  zip pullover", 'the network server room is filled with servers stock photo', 'luggage sale advert with suitcases', 'volvo v40 estate 2 0 manual, 5 door hatch'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00011503', __restore_key__=('MapDataset', 269, 'Webdataset', 'pretrain-000001.tar', 60), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...home and', 'adidas swifteloe prime knit sneakers', 'a black torys wedge with a bow on top', 'a truck with a flatbed and a small rear hitch hitch', 'the copper bracelet is made from old silver bells'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00002492', __restore_key__=('MapDataset', 270, 'Webdataset', 'pretrain-000004.tar', 51), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...he three tier cake is topped with fresh flowers and is also adorned with succulent foliage', 'an abandoned steel mill in the fog with large amounts of light', 'canon flashes with the speedlite gn60'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00004808', __restore_key__=('MapDataset', 271, 'Webdataset', 'pretrain-000003.tar', 46), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...r decoration', 'a person with an achilles pain on their foot', '25 delicious vegan desserts and snack ideas', 'the power of love night race at m bombbolt stadium', 'private equity as an asset class'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017782', __restore_key__=('MapDataset', 272, 'Webdataset', 'pretrain-000003.tar', 47), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...lue leather shopping tote with fringes', 'murder room a novel audiobook', 'phil collins hits by phil collins', 'green glitter nail polish with glitter flakes', 'an image of four square marble bases'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00001415', __restore_key__=('MapDataset', 273, 'Webdataset', 'pretrain-000000.tar', 57), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ascar daytona bay', 'sheryl fisher speaking at the 2019 milk forum', 'an image of an aluminum hose fitting adapt with a male thread', 'the front yard is a focal for a backyard play area in savannah'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00004734', __restore_key__=('MapDataset', 274, 'Webdataset', 'pretrain-000003.tar', 48), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ody shop products on a pebble beach', "doctor who's doctor's amy and the doctor", 'the unlined dressing gown', 'polka dot print top', 'the new black panther poster', 'the casino floor at the casino'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00000060', __restore_key__=('MapDataset', 275, 'Webdataset', 'pretrain-000001.tar', 61), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ads', 'a beautiful view of the beach golf course at los estancho beach golf course in santa ana', 'the logo for two mobile apps with the text, automate content creator plus and the icon of a pencil'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00001268', __restore_key__=('MapDataset', 276, 'Webdataset', 'pretrain-000003.tar', 49), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...urtney cox', 'bacon on a foil tray sitting on top of a sheet of foil', 'the browning 1878 rifle has a mahogany body and brass barrel', 'a model wearing the missoni space dye pant in grey and orange'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00019173', __restore_key__=('MapDataset', 277, 'Webdataset', 'pretrain-000003.tar', 50), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...t of the taj mahal mosque in agra, india', 'santa claus is seen here in this undated file photo provided by sony pictures', 'passengers on the back of a ship with snow and ice floes in the distance'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00022845', __restore_key__=('MapDataset', 278, 'Webdataset', 'pretrain-000003.tar', 51), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... products website", 'the view from the right field wall during a chicago cubs game', '3d front elevation of contemporary house modern house plans', "a mugshot from a man's face in a police portrait"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00016629', __restore_key__=('MapDataset', 279, 'Webdataset', 'pretrain-000004.tar', 52), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... comby infant carseat with car seat cover', 'a close up of a robot spirits gundam figure in green and white', 'the silhouette of a man in a hoodie with data symbols', '5 pc dining set with 4 chairs'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017529', __restore_key__=('MapDataset', 280, 'Webdataset', 'pretrain-000001.tar', 62), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'an induction range with six plates of various food items', 'a square corrugated paper packaging box with vegetable print', 'jay ramsey anthology vol 1', 'a blue tool kit with tools for hand work'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00014301', __restore_key__=('MapDataset', 281, 'Webdataset', 'pretrain-000002.tar', 55), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...igo denim jeans charcoal', 'the north face mountain parka with a fur - lined hood, coyote brown', 'leopard print handbag', 'a small tower fan, with timer, humidity control, remote control and timer'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00013586', __restore_key__=('MapDataset', 282, 'Webdataset', 'pretrain-000003.tar', 52), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...r children's educational program and kids toy, toys and toy store, isolated", 'man and woman in masks corona viruses fly around them vector illustration for advertising poster, brochure products on'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00008797', __restore_key__=('MapDataset', 283, 'Webdataset', 'pretrain-000000.tar', 58), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...for an office', 'anora coat with det pockets and fur trim', 'the info display in the front seat of the new vw', 'an annoying orange oranges orange battle of the fruit games', 'monogram heart bangle'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00011656', __restore_key__=('MapDataset', 284, 'Webdataset', 'pretrain-000000.tar', 59), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... that says i love my glaupies', 'a pair of william ii carved chairs', "a photo of a man's face with text, self confidence is the first requisite to great undertaking", 'fire hydrant covers in china'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00001718', __restore_key__=('MapDataset', 285, 'Webdataset', 'pretrain-000004.tar', 53), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...k evening dress with leopard print detail on the top and sheer back', "spurs forward person is seen celebrating scoring his side's third goal in a champions match", 'the medieval lady in chain mail'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002851', __restore_key__=('MapDataset', 286, 'Webdataset', 'pretrain-000003.tar', 53), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...t for ball with philadelphia eagles cornerback donovan brown', 'taking his shot hot jocks 7 by kendall ryan', 'water floods in a neighborhood with houses and the sky reflected by the flooded street'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00003778', __restore_key__=('MapDataset', 287, 'Webdataset', 'pretrain-000002.tar', 56), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rm on stage', "the police insignia on a police officer's uniform", 'seo keyword research and seo competitor analysis', "the double - tailed giraffe kids t - shirt light blue by katie kennedy's shop"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003038', __restore_key__=('MapDataset', 288, 'Webdataset', 'pretrain-000001.tar', 63), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...kground illustration', 'a water plane flying low to the ground on a body of water', "the girls at the table at the girls'birthday party", "valentine's day tags, valentine's cards - pattern and clip"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00009917', __restore_key__=('MapDataset', 289, 'Webdataset', 'pretrain-000003.tar', 54), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 'crepe silk trousers with buttons', 'tie dye flared skirt, flare skirt, long skirt, bohohoho, festival skirt', 'christmas sparkles on a colored background', 'a cute and colorful girls pageant gown'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007391', __restore_key__=('MapDataset', 290, 'Webdataset', 'pretrain-000002.tar', 57), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ackground', 'hotsale goggles, snow sports goggles with anti - fog lens, snowmobile driving goggle', 'the jock quilt project by john mangaane', 'baby boy moan themed baby shower invitation printable'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00005713', __restore_key__=('MapDataset', 291, 'Webdataset', 'pretrain-000000.tar', 60), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ee in the artisan style', 'the rifles of the 1st army regiment are mounted in a trench at the beginning of the battle of ypres', 'the old courthouse pub and gardens at the entrance to the town park'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00019585', __restore_key__=('MapDataset', 292, 'Webdataset', 'pretrain-000004.tar', 54), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...yes', 'a variety of foods arranged in small wooden bowls stock images', 'a man stands in front of the fish section in a supermarket', 'a bride and groom share their vows at a beach wedding ceremony'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00018676', __restore_key__=('MapDataset', 293, 'Webdataset', 'pretrain-000004.tar', 55), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...with the word poet in front of a blue background with healthcare icons', 'what does grat mean? it stands for granator retained annuity trust', 'a cover for a book with the title murder at the opera'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017892', __restore_key__=('MapDataset', 294, 'Webdataset', 'pretrain-000001.tar', 64), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...blue hexagonals', 'black 4 tiers trolley with drawers', 'a swan on a lake with blue water christmas ornaments', 'hot selling virgin virgin human hair weft', 'deutschland distressed german crest mug'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00008445', __restore_key__=('MapDataset', 295, 'Webdataset', 'pretrain-000002.tar', 58), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'a pair of cards featuring the red truck card and die - cuts vehicle', 'eva pad, eva film interlayering samples for safety glass lam', 'buy mango indigo skinny jeans, medium blue online at johnle'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00014586', __restore_key__=('MapDataset', 296, 'Webdataset', 'pretrain-000003.tar', 55), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...items, toothbrush and a bowl on yellow background with dots royalty illustration', 'a small tin of pink soy wax with a dog icon on it', 'isolate amino 1000 caps scitec nutrition', 'girls skirt grey'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00005274', __restore_key__=('MapDataset', 297, 'Webdataset', 'pretrain-000004.tar', 56), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...2013', 'a tobacco field under a cloudy sky', 'purple polka nails and an accent of black', 'vanilla sticks with a single flower', "a new inter milan shirt designed by nike and the team's new apparel"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00007020', __restore_key__=('MapDataset', 298, 'Webdataset', 'pretrain-000004.tar', 57), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ot a game plan for you '", 'the totally sweet 90s book is a fun and educational family - sized guide to childhood memories', "a home in boca grande, fl that's a nice, well - maintained neighborhood"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00008138', __restore_key__=('MapDataset', 299, 'Webdataset', 'pretrain-000003.tar', 56), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...g nation on further easing of lockdown', 'diy flower ideas for paper roses and other crafts', '2009 bmw x5 awd with navigation system', 'operations manager resume', 'delcate dress by cathayy at tsr'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007264', __restore_key__=('MapDataset', 300, 'Webdataset', 'pretrain-000002.tar', 59), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...na, korea and japan, including global lng imports', "an mri specimen is taken from a computed x - ray image from this patient's chest", 'gladator vs ci 42 falcon - 1943', 'ddram 512gb 184p pc2280c3'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00011863', __restore_key__=('MapDataset', 301, 'Webdataset', 'pretrain-000000.tar', 61), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...wedding band with an engraved engagement ring', 'an artist painting frida gutierrez', 'small plastic playhouse with green roof', 'three green and purple toad mushroom earrings', 'dirty spot remover'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00003817', __restore_key__=('MapDataset', 302, 'Webdataset', 'pretrain-000004.tar', 58), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...pies', 'a dress or something? mesh dress', 'a bowl of hot chicken dip with chips on a plate', 'person at an event for captain america civil war', 'the villagers in a movie poster for village people'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00006882', __restore_key__=('MapDataset', 303, 'Webdataset', 'pretrain-000001.tar', 65), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...d prix', "a picture of the cover of a book, alice's adventures in wonderland by lewis carroll", "the world's most exciting world - sloping strategies context clues morphology roots prefix, suffixes"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00000050', __restore_key__=('MapDataset', 304, 'Webdataset', 'pretrain-000003.tar', 57), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e page with the click here to uncover food bloggers online course videos', 'sodastream is coming to a summer house party', 'the poster for the youth outreach event is the only poster you will need'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00009556', __restore_key__=('MapDataset', 305, 'Webdataset', 'pretrain-000001.tar', 66), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...the word nap time is 14 22 on it', 'josephers kress tie neck blouse', 'three hp products for a range of printers', 'this is an example of a simple wedding date sign', 'the install page in windows 7'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00022799', __restore_key__=('MapDataset', 306, 'Webdataset', 'pretrain-000002.tar', 60), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eaven'n den usa written on it", 'the narrow alleyway in the old town of dubrovsk', 'the new girl star brookeaus and her fiance scott porter', 'two men and a woman stand next to a camouflage blanket'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00010481', __restore_key__=('MapDataset', 307, 'Webdataset', 'pretrain-000002.tar', 61), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...he hoop ball with jon kavanagh', 'the map shows the location of a location of the french fishing port of courche', '2012 chrysler grand voyager', 'french flup', 'job responsibilities for hr manager'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00002174', __restore_key__=('MapDataset', 308, 'Webdataset', 'pretrain-000000.tar', 62), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ard cushion is made from 100 % polyester with a large, feather - back pillow in various shades', 'the beautiful white mink fur hat is made from real fur and it is the perfect accessory for any kind'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00016733', __restore_key__=('MapDataset', 309, 'Webdataset', 'pretrain-000001.tar', 67), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... is displayed against a white background', 'easter bunny craft with cotton balls and paper plate', "le vian ladies's 18k gold sapphire and diamond band", 'the nuvo audio nuvox dj controller for djs'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00010451', __restore_key__=('MapDataset', 310, 'Webdataset', 'pretrain-000003.tar', 58), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...classic car grills in blue framed print', 'a tray full of christmas treats', 'coffee cups, with a spoon and a sugar bowl a royalty illustration royalty stock illustration', 'sushi platter at person'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00020833', __restore_key__=('MapDataset', 311, 'Webdataset', 'pretrain-000002.tar', 62), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... tough for the nrl premiership', 'the material gallery app for the lg nexus, showing what to do on your phone', 'two faces of president donald - trump and republican republican candidate jeff davis'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002897', __restore_key__=('MapDataset', 312, 'Webdataset', 'pretrain-000003.tar', 59), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...tion with empty rails photo', 'director ron howard and film crew in the library', 'a couple walk along an brick sidewalk in old town philadelphia', 'asian babe gets naked in the rain on the balcony'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00002530', __restore_key__=('MapDataset', 313, 'Webdataset', 'pretrain-000003.tar', 60), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...october 16 2013', 'cheerleaders for the ucla ucla nuggies cheer team perform during the ucla ucla bruins versus arizona wildcats', 'the bale view room balcony at the la jolla motel in riverside, ca'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00012054', __restore_key__=('MapDataset', 314, 'Webdataset', 'pretrain-000000.tar', 63), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nimal countings sheet', 'stickman illustration of a person crossing the street', 'earth tree in the form of a planet map with heart shapes and grass on the background', 'redvalent dress, with heart'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00008265', __restore_key__=('MapDataset', 315, 'Webdataset', 'pretrain-000002.tar', 63), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...y field', 'pluto the dog cartoon cute puppy dog clip art png download', 'some people standing in the water doing a water safety course', 'woman with purple hair and glasses is making a stop gesture'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00022561', __restore_key__=('MapDataset', 316, 'Webdataset', 'pretrain-000001.tar', 68), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ing against the side of it', 'a woman using the new asus zenbook on her table, with the phone and tablet in the background', 'a cup of caramel ice cream and chocolate chips on top of a wooden table'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00021580', __restore_key__=('MapDataset', 317, 'Webdataset', 'pretrain-000004.tar', 59), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...rnate, toiler style background', 'the red and black office lobby with red accented walls, a concrete floor, and several chairs, a', 'a man and a woman talking on the street during a christmas event'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00003549', __restore_key__=('MapDataset', 318, 'Webdataset', 'pretrain-000003.tar', 61), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...nds by tenure of adults, by occupation', "man's hand holds a white sheet with the word diet on it", 'watercolor world map paint splatter by art galaxy', 'pria chopra with anil kumar during the show'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00023110', __restore_key__=('MapDataset', 319, 'Webdataset', 'pretrain-000002.tar', 64), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ustration of a book festival banner with colorful books', 'ohio ohio osp theme with a theme background', 'the new cat machinery show at the 2019 consumer hardware show', 'a 3d car on a grey surface'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00001689', __restore_key__=('MapDataset', 320, 'Webdataset', 'pretrain-000001.tar', 69), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...bench has been painted white, and the shelves are lined with mud and wood hooks', 'the black velvet jumpsuit from person is on sale for $ 99', 'prayer wheel in a buddhist temple, yangshushan, china'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004247', __restore_key__=('MapDataset', 321, 'Webdataset', 'pretrain-000004.tar', 60), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... table with some ice', 'natalie roberts shopping with her nieces 072481358', 'wolverine weapon uncanny 4', 'a large metal sphere sculpture on top of a stand', 'the nourishing hair treatment product'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00017515', __restore_key__=('MapDataset', 322, 'Webdataset', 'pretrain-000001.tar', 70), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eam duo', 'an image of a brown and white bulldog sticker', 'white folding table with two shelves', 'the d concert micro sdhc memory card with sd adaptor', 'a pens and pencil set of a red maple wood'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00017346', __restore_key__=('MapDataset', 323, 'Webdataset', 'pretrain-000002.tar', 65), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... 'a colorful abstract fire and ice pattern stamp', 'the brothers brother sun sister moon, dvd', 'the bradford disney crystal snowman figure from johndeear', 'an irobot robot with a white background'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00011781', __restore_key__=('MapDataset', 324, 'Webdataset', 'pretrain-000002.tar', 66), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...as a leather texture finish', 'a sales invoice that is very useful for business and', 'july calendar in red and white', "an image of the earth moon rising book, which reads,'the earth moon rising '"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000863', __restore_key__=('MapDataset', 325, 'Webdataset', 'pretrain-000000.tar', 64), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... in a glass with whipped cream and pistachio cheese', 'a highway winding up the side of a mountain with green valleys and road markings', 'the drifters band | fort walton, tx | cover band | photo 7'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00008668', __restore_key__=('MapDataset', 326, 'Webdataset', 'pretrain-000003.tar', 62), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...his big ass', 'the green jade mala mala with buddha charm', 'wheat grain in a napkin with word spelt', 'the letter h on the back of a red fire hydrant with chrome letters and emblems shower curtain'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009866', __restore_key__=('MapDataset', 327, 'Webdataset', 'pretrain-000000.tar', 65), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...oween decor ideas for the family to steal', 'slots online casino book of ra', 'three stuffed corn dogs with bacon and peppers', 'an archety wall with succulent wreaths', 'mustard designer anam gown'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00013632', __restore_key__=('MapDataset', 328, 'Webdataset', 'pretrain-000000.tar', 66), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...- neck long", 'the crafty within hearts valentine card', 'ibiza 2011 the closing part 1 deluxe edition 2010', 'actors emily blunte and adam brody at the red carpet of the 2012 toronto film festival'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00006702', __restore_key__=('MapDataset', 329, 'Webdataset', 'pretrain-000003.tar', 63), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...old tongue', 'girls wearing colorful powder paint running with text overlay what to wear at a color powder run', "grandfather's view in acadias, cape breton national park, nova nova nova nova duvet"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00011255', __restore_key__=('MapDataset', 330, 'Webdataset', 'pretrain-000000.tar', 67), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... steel teeth', 'free shipping 2013 baby hoodies children long sleeve clothes autumn and winter boys and girls hoodie children clothing', 'bunny art print, wall art poster, rabbit nursery decoration'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00019724', __restore_key__=('MapDataset', 331, 'Webdataset', 'pretrain-000000.tar', 68), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag..., 'the address to the dubai hotel in abu', 'an image of a car parked on a cobblestone driveway', 'an antique gasoline flying horse sign at a station', 'a graphic shows how a water test device works'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00010705', __restore_key__=('MapDataset', 332, 'Webdataset', 'pretrain-000000.tar', 69), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...thank you', 'this is the edit button in microsoft paint', 'a blue ocean strategy', 'the raven woman and the black crows poster', "the edmonton oilers'red, white and blue striped captain cap is seen"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00004442', __restore_key__=('MapDataset', 333, 'Webdataset', 'pretrain-000004.tar', 61), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...patio furniture sets', 'race winner jamie penshaws and wife kim', 'the concise greek - english lexion of the new testament', 'a navy and brown tweed newsboy hat with an orange striped check pattern'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00019399', __restore_key__=('MapDataset', 334, 'Webdataset', 'pretrain-000002.tar', 67), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...er callum brown celebrates after his own goal during the first half of the match', 'an image of barry adams with blonde hair and blue eyes', 'the hong v new zealand match is postponed at rugby park'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007093', __restore_key__=('MapDataset', 335, 'Webdataset', 'pretrain-000002.tar', 68), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e with two buckles on the front', 'black ninja boy costume', 'colorful lifeguard booth on miami beach with blue sky in the background', 'kate winslet in a white gown at the 2013 golden globe awards'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00010193', __restore_key__=('MapDataset', 336, 'Webdataset', 'pretrain-000000.tar', 70), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...erature for 12 years', 'a model of an american airlines flight', 'the large intestion and the large intestion illustration of a picture of the large intestion', 'graphing linear equations worksheet'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00016546', __restore_key__=('MapDataset', 337, 'Webdataset', 'pretrain-000001.tar', 71), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ion on a table', 'an antique silver ear hoop and three opal stone nose ring', 'a clear, flexible, high density poly tubing hose', 'wireless bluetooth wireless fabric speaker speaker with microphone'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00000348', __restore_key__=('MapDataset', 338, 'Webdataset', 'pretrain-000001.tar', 72), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...d, which includes minsty styling products and other products', 'an irish leprezi with the word risk', 'the international manufacturing technology show in chicago 2018', '12 food gifts for christmas'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00007528', __restore_key__=('MapDataset', 339, 'Webdataset', 'pretrain-000002.tar', 69), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... executive resume', 'a laptop is being held by a pryant to the top with a screw', 'katrina and naver at a press conference in mumbai', 'several pepper pods on a metal shelf with a tray holding them'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00013713', __restore_key__=('MapDataset', 340, 'Webdataset', 'pretrain-000002.tar', 70), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ed hills state park in john day fossil national monument, oregon', 'vintage white driving gloves with silver hardware', "jane shrvan's song for your valentine, from the motion to a revolution album"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00018734', __restore_key__=('MapDataset', 341, 'Webdataset', 'pretrain-000000.tar', 71), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ighlighted', 'the featured articles of the bike magazine and feature on the internet', 'beekeeping experts inspect a hive on the honey farm in pennsylvania', 'a frog silhouette set in various poses'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00022744', __restore_key__=('MapDataset', 342, 'Webdataset', 'pretrain-000004.tar', 62), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...tournament', 'the 18th hole at plantation club, with two green and fountain', 'people are seen outside of the vodafone store in london', 'an x - ray view of the skeleton of the respiratory on black'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00010630', __restore_key__=('MapDataset', 343, 'Webdataset', 'pretrain-000003.tar', 64), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...on the tail', 'stainless steel panel from bosch refrigerator door', 'some information from a store owner', 'marlin magazine cover', 'anusha basra wearing a pink sari dancing at the press conference'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00009355', __restore_key__=('MapDataset', 344, 'Webdataset', 'pretrain-000002.tar', 71), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...for an interview paper is shown', 'the city of sao parana is seen from above', 'a snow machine in front of a house with wreaths and bows', 'a colorful birthday greeting with colorful wrapping boxes'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00000766', __restore_key__=('MapDataset', 345, 'Webdataset', 'pretrain-000001.tar', 73), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...the cover of the book, a practical guide to catastrophic brain injury claims', 'a bride and groom posing for a photograph in a winery cellar', 'a mclaren p1 on display at the 2010 detroit auto show'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00001741', __restore_key__=('MapDataset', 346, 'Webdataset', 'pretrain-000004.tar', 63), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...n smiling, one in a flower cart and the other holding bouquets', 'a silver mercedes g550 with 22 inch voss wheels', 'cowboys leading the way on the streets in stockyards station, the main city town'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00004514', __restore_key__=('MapDataset', 347, 'Webdataset', 'pretrain-000000.tar', 72), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e on a yoga mat', 'a pink and black painting in the style of grungy phone case - design by humans', 'steve filz, right, with golden state coach jim davis at the warriors media conference in oakland'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00018818', __restore_key__=('MapDataset', 348, 'Webdataset', 'pretrain-000001.tar', 74), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...am 3 and 5 for the antenna cables', 'a pillar box, red, in a white background', 'the metallic copper foam foams give the foam a rich glow', 'an old red barn in the countryside of virginia art print'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007799', __restore_key__=('MapDataset', 349, 'Webdataset', 'pretrain-000003.tar', 65), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... orange kitchen and dining room set up in a room with orange floors', 'the front view of a waterfront home at the shoreline on the river in the spring', 'a wake boarder at the speed board in a lake'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00007366', __restore_key__=('MapDataset', 350, 'Webdataset', 'pretrain-000003.tar', 66), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...onoring the 75th anniversary of the theatre guild and musical festival", 'singer katy - perry speaks about her tour with the band', 'a lecture by the university of california at san diego uc irvine'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00023743', __restore_key__=('MapDataset', 351, 'Webdataset', 'pretrain-000004.tar', 64), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...h at club monaco on november 9 2011', 'ponto bridge over the arno river in the historical center of florence, italy', 'the asus rog strix gx514w is a budgety gaming laptop', 'supernatural happening'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009744', __restore_key__=('MapDataset', 352, 'Webdataset', 'pretrain-000000.tar', 73), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ck wedding garter, toss', 'the terms for an anonymous privacy message', "a collage of baby's nursery items", 'rizzoli and isles complete fifth season dvd', 'black cotton jumpsuit with waist pockets'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00020253', __restore_key__=('MapDataset', 353, 'Webdataset', 'pretrain-000003.tar', 67), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... composite of two soccer players with the logo of hull city and a close up image of a tiger', '3 braid hairs tutors for medium hair youtube', 'tattoos tribal tattoos designs and meanings for tribal'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00005790', __restore_key__=('MapDataset', 354, 'Webdataset', 'pretrain-000001.tar', 75), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...iphones side by side with a payless card on the screen, and the bank app and in', 'the oneplus one is shown in different colors and features', "women's jersey tunic top, boat neck, lightweight knit"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00010589', __restore_key__=('MapDataset', 355, 'Webdataset', 'pretrain-000004.tar', 65), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...dwill logo', 'three lavender, lila and pink flower bridal clutch wedding bag, brides purse, bridal purse', 'american tiger martial arts', 'a man standing under a ceiling display filled with flowers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00003353', __restore_key__=('MapDataset', 356, 'Webdataset', 'pretrain-000001.tar', 76), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...and long sleeved trousers', 'cigarettes stacked up on display at the tobacco company', 'a man doing barbell workout exercises in the gym', 'smoke detector for home surveillance with motion detector'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00012699', __restore_key__=('MapDataset', 357, 'Webdataset', 'pretrain-000000.tar', 74), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...stration', 'jockey with his horse at the weighing during event', 'the samsung galaxy s4 camera with its full frame lens', 'dynojt power commander 3 usb - 2004 suzuki intruder 800e dynojte slave / e'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00004334', __restore_key__=('MapDataset', 358, 'Webdataset', 'pretrain-000001.tar', 77), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ns', 'the openoff office page, showing an icon of a person in the job search window', 'raj kapoor dance dance with sonakshi khan', 'the moomins and the great flood', 'the great barrier reefs poster'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000761', __restore_key__=('MapDataset', 359, 'Webdataset', 'pretrain-000000.tar', 75), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...in burger book promotion is on the front of a dark backdrop, with a picture of two books', 'the art of uncanny mission marvel villains, person, comic book covers, graphic novels, comics, book worms'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00007408', __restore_key__=('MapDataset', 360, 'Webdataset', 'pretrain-000004.tar', 66), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...frontier', "the little tiff rabbit rabbit ears child's hand mirror with hanger", 'rear coiling and shock spring assembly for the mercedes 600d', 'a large walon is in the water with a very long tusk'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00005533', __restore_key__=('MapDataset', 361, 'Webdataset', 'pretrain-000003.tar', 68), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... display at the national railroad museum in carson, wyoming canvas print', 'hoodie zip through - veste en sweat - black / red', 'the saturdayss singer lucyl lennix in a cropped top and floral skirt'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00011606', __restore_key__=('MapDataset', 362, 'Webdataset', 'pretrain-000002.tar', 72), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...gearwheels for the team associated helica defender', 'an impression of the sun, watercolor by kathy doey', 'a woman wrapped in towels stock photo', 'two large sterling silver bangles with rope wire'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00023485', __restore_key__=('MapDataset', 363, 'Webdataset', 'pretrain-000000.tar', 76), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...round with light rays stock photo', 'business dashboard showing job listings, salaries and jobs', 'a woman is looking at a shoe display', 'an example message from the messenger user on a cell phone'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00017443', __restore_key__=('MapDataset', 364, 'Webdataset', 'pretrain-000000.tar', 77), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...invaders game', 'a man looking in the mirror with the devil silhouette royalty - fotografico', 'an unifi network proxy with a message for vpn server', 'underfloor floor heating kit with accessories'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00009375', __restore_key__=('MapDataset', 365, 'Webdataset', 'pretrain-000003.tar', 69), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e that is intended for an office work', 'hostas, in the summer months with colorful leaves', 'a - strap sandals for boys by quikskill', 'the war memorial at camp, virginia image click for full size'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00008002', __restore_key__=('MapDataset', 366, 'Webdataset', 'pretrain-000003.tar', 70), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...of a cardboard background', 'a ball on top of a field with the text total goals', 'shiving with the hindu identity and the bjp party', 'provence 2 door buffet sideboard with glass doors and drawers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00002749', __restore_key__=('MapDataset', 367, 'Webdataset', 'pretrain-000004.tar', 67), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ather skirt outfit to try out this winter', 'a woman wearing a black top, grey vest, gray jeans, and pumps', "hundreds of people gather to protest for the us embassy at mexico's consulate in london"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00019592', __restore_key__=('MapDataset', 368, 'Webdataset', 'pretrain-000000.tar', 78), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...e seen behind a line of pink washed buildings footage', 'debate moderators sit in front of podiums with empty glasses on', 'the parental payoff question page on the parental payoff question website'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00009003', __restore_key__=('MapDataset', 369, 'Webdataset', 'pretrain-000000.tar', 79), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...occupational therapist's guide to home modifications and practice", 'christmas tree card hand drawn in retro style', 'coastal kids - the best of sea world', 'the philips hue white smart light bulbs'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00007423', __restore_key__=('MapDataset', 370, 'Webdataset', 'pretrain-000000.tar', 80), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ent to a line tangent to the point of tangency', 'a maharaja standing in front of the golden pot in jaipur palace', 'the clouds in this afternoon sky are very beautiful the houses are also well lit'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00016861', __restore_key__=('MapDataset', 371, 'Webdataset', 'pretrain-000000.tar', 81), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...god bumper sticker', 'the etched clear frost tumbles are a white plastic tumble with gold lettering', 'a round bread sitting on top of a parchment surface', 'you make me feel all muggie inside card'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00008177', __restore_key__=('MapDataset', 372, 'Webdataset', 'pretrain-000004.tar', 68), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...a large space station', 'the gazele in the park at boston common canvas print', 'white lace sash wedding dress with capped sweetheart scoop neck', 'the right light is not the only way to use the ir'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00006852', __restore_key__=('MapDataset', 373, 'Webdataset', 'pretrain-000004.tar', 69), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ty algorithm applied to", 'what is gesture drawing?', 'the linen cabinet is organized by placing the linens on top of the shelf and holding', 'the 2013 mazda mazda 3 - door sedan parked in an alley'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00003500', __restore_key__=('MapDataset', 374, 'Webdataset', 'pretrain-000000.tar', 82), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...sitting in front of an organ pipe in a church', 'the kinks in concert at red rocks, camden', 'a hotel room in the marriott resort', 'a baking sheet with several peanut butter chocolate chip cookies'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00016721', __restore_key__=('MapDataset', 375, 'Webdataset', 'pretrain-000001.tar', 78), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...photo provided', 'a dial indicator for micrometers, including digital display', 'blue and gold cloud iii canvas print or poster', 'blue ceramic c7 christmas lights set with white wire on green wire'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00000393', __restore_key__=('MapDataset', 376, 'Webdataset', 'pretrain-000000.tar', 83), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...a 1987 pontiac montero that we drove, which had an xr1 engine', 'milwaukee electric m10r series 3 0 ton overhead chain hoist with power cord', 'battery monitoring software for windows and macintosh'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00000129', __restore_key__=('MapDataset', 377, 'Webdataset', 'pretrain-000001.tar', 79), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...collage of colonial creates the flag in the video', 'christmas trail mix cookies with chocolate and nuts', 'an updated ad for the student survey', "the strap is attached to the watch's watch holder"])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00004633', __restore_key__=('MapDataset', 378, 'Webdataset', 'pretrain-000001.tar', 80), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... is an image of a metal rolling pan rack', 'the best dressed moments from cfny 2013', 'a heatmap for a house with a heat map', 'the little cat, the geronimo stilton and the cherry blossom adventure'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00005888', __restore_key__=('MapDataset', 379, 'Webdataset', 'pretrain-000001.tar', 81), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...erseys', 'a blue paper with the title, equality, diversity and inclusion strategy', 'the catholic parish of siensa, s c, event calendar 2012 - 2013', 'brazil soccer team in the world cup semifinals'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00020868', __restore_key__=('MapDataset', 380, 'Webdataset', 'pretrain-000000.tar', 84), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... bucks'gerald davis and jimmy butler", 'the fuji fuji announces the fuji m1 in black', 'a game screen showing two herculess in the cave', 'a desktop app showing the time in the agenda of the school'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00013027', __restore_key__=('MapDataset', 381, 'Webdataset', 'pretrain-000003.tar', 71), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ampler featuring four different patterns on top of each other', 'the necklace is made of beads of light green peridoil', 'an open document and the words, changes of document in the approval process'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00007992', __restore_key__=('MapDataset', 382, 'Webdataset', 'pretrain-000001.tar', 82), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...he fastest rapp on the world's highest speed test track", 'two nokia lumia cases, orange, are held in hand', 'the pixel survival story of one pink pig in a minecraft world', 'the wedding point logo'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00005340', __restore_key__=('MapDataset', 383, 'Webdataset', 'pretrain-000001.tar', 83), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...', "a naturalist's guide to the insects of britain and northern europe by steve macdonald", 'zip code 81706, texas real estate house value index', 'pleated trim blouse', 'the cord skirt, brown sued'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00001389', __restore_key__=('MapDataset', 384, 'Webdataset', 'pretrain-000001.tar', 84), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ok covers with a cartoon soldier running from the otherworld', 'the 1 000000 scale diagram for a fountain font', 'all the kings men, a british soldier from the restoration to waterloo by paul david'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00003851', __restore_key__=('MapDataset', 385, 'Webdataset', 'pretrain-000000.tar', 85), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...eviewing the markings for a construction site', 'the ruins of st andrews castle', 'a plane flying over the palm islands', 'an image of the original projector lamp module for sony / sony z1 / zep840'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00019612', __restore_key__=('MapDataset', 386, 'Webdataset', 'pretrain-000002.tar', 73), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...unning car', 'kendra ellis talking to otrc after the dancing competition', 'the hong cityscape with lots of tall buildings from high up', 'the water fight on campus is a must, even after graduation'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00017841', __restore_key__=('MapDataset', 387, 'Webdataset', 'pretrain-000003.tar', 72), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...gan sweater with vintage glasses', 'a tinova tinovo wood base oil', 'usb cable cord to printer for computer', 'a cartoon mouse collection in a variety of poses', 'the nfl dog collar with bell bells'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00002798', __restore_key__=('MapDataset', 388, 'Webdataset', 'pretrain-000001.tar', 85), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...on attends the endorsement ceremony of person in beijing china on july 20 2019', 'the 2013 bmw z4 roadsport is shown', '2010 topps allen roms - before there was tops - red paper frame tp22 - al wes'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00000794', __restore_key__=('MapDataset', 389, 'Webdataset', 'pretrain-000004.tar', 70), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...son and singer person perform on stage during the 2014 person concert at the american airlines', 'a woman in a hat with a stick and carrying a green shopping bag on the street in paris stock photos'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00015296', __restore_key__=('MapDataset', 390, 'Webdataset', 'pretrain-000003.tar', 73), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... alkaliize batteries 4 pack', 'top mount stainless steel kitchen sinks stainless steel kitchen sink, farmhouse style sinks', 'two rows of wire and galvaniized grating, each laying on top of a stack'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00000572', __restore_key__=('MapDataset', 391, 'Webdataset', 'pretrain-000003.tar', 74), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ain and his wife, barbara mccain', 'medicare enrollment form on a desk next to a pen and glasses', 'the dashboard inside the 2019 dodge rambler', 'the way to do it black long sleeve romper at lulus'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00014714', __restore_key__=('MapDataset', 392, 'Webdataset', 'pretrain-000004.tar', 71), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...s business three piece jacket and pants suit", "the screen for the slot slot slot, showing the game's name and the unlock code,", 'the sun sets on a rocky river in the canadian rockies canvas print'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000002.tar/ps_00019175', __restore_key__=('MapDataset', 393, 'Webdataset', 'pretrain-000002.tar', 74), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...th his feet up looking out the window', 'a tyranaurus with the logo and the words rolar labs on it', 'an image of people on the wheel of death ride in a coaster coaster coaster coaster coaster cars'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00006473', __restore_key__=('MapDataset', 394, 'Webdataset', 'pretrain-000000.tar', 86), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...mograph by horace grove dennis', 'a black and white kitchen with an orange cabinet', 'a large group of people in the shape of an aon with their arms in the air', 'folk designs and motifs from india'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000003.tar/ps_00023701', __restore_key__=('MapDataset', 395, 'Webdataset', 'pretrain-000003.tar', 75), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...new ringtone on android smartphone', 'the cover for carnival row, starring two male protagonists', 'australian batsman michael clarke is ready to make a big impact on england in the opening session'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00021649', __restore_key__=('MapDataset', 396, 'Webdataset', 'pretrain-000001.tar', 86), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...net of the apes trailer', 'the modern bar stool with two x back bars', 'a man standing in front of a window unit', 'a minecraft screenshot of an alley', 'the missing person in the missing fish case'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00000316', __restore_key__=('MapDataset', 397, 'Webdataset', 'pretrain-000004.tar', 72), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...astic wall, covered with air bubble wrap, and people standing near it', "king diamond the spider's lullaby", 'a variety of invitation samples from card and pocket for watercolor wedding invitations'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00012910', __restore_key__=('MapDataset', 398, 'Webdataset', 'pretrain-000001.tar', 87), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag... with baubs for the christmas greeting - stock vector', 'chinese style hand fan bamboo wood folding bamboo hand fan chinese dance fans', 'the shirt from the back, with the green stripe at the front'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00007606', __restore_key__=('MapDataset', 399, 'Webdataset', 'pretrain-000000.tar', 87), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...plant, illustrating how it is built', 'consumer reports magazine cover featuring a person holding a smartphone in front of washing machines', '2 - port video switch, a power switch, and a surge box'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000000.tar/ps_00002284', __restore_key__=('MapDataset', 400, 'Webdataset', 'pretrain-000000.tar', 88), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...h smoked salmon, prosciut, and dilled dilllie', 'gang leader for a day a rogue sociologist takes to the streets by sudhir venkatesh', 'a charlie brown christmas live wallpaper', 'little village set'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000001.tar/ps_00006873', __restore_key__=('MapDataset', 401, 'Webdataset', 'pretrain-000001.tar', 88), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...ly toward each other', 'a - line / princess v - neck floor - length chiffon prom dress with ruffle lace', 'the pamphlet for a page that shows the process for spray and cyclr corrosion test chambers'])
----------
Traceback (most recent call last):
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/map_dataset.py", line 150, in __iter__
    for idx, (sample_idx, inner_sample) in enumerate(
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/wrappers/base.py", line 182, in iter_ctx
    x = next(it)
  File "/home/rana.zayed/.conda/envs/llava-ov-4b-clean/lib/python3.10/site-packages/megatron/energon/task_encoder/base.py", line 168, in seed_wrapper_generator
    sample = next(it)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 201, in encode_sample
    l_sample_packed = self.pack_selected_samples(l_Qwen2VLImageTaskSample)
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/qwen2vl_task_encoder.py", line 554, in pack_selected_samples
    super().pack_selected_samples(samples),
  File "/l/users/rana.zayed/new_fastvlm/LLaVA-OneVision-1.5/aiak_training_llm/data/multimodal/task_encoder.py", line 346, in pack_selected_samples
    sample = sample[:max_length]
TypeError: 'Qwen2VLImageTaskSample' object is not subscriptable
----------
PackedCaptioningSample(__key__='pretrain-000004.tar/ps_00003554', __restore_key__=('MapDataset', 402, 'Webdataset', 'pretrain-000004.tar', 73), __subflavor__=None, __subflavors__={}, images=[<PIL.Imag...een and white print onesuit', 'a vehicle door showing the rear door glass', 'jason donaldson hits out of the bunker in the first round of the honda lpm on monday', 'the 5 most fights in ufc history'])
----------
